---
title: "Transforms and distributions"
engine: julia
---

```{julia}
import Random
Random.seed!(468);

using Distributions: Normal, LogNormal, pdf, logpdf, Distributions
using Statistics: mean, var
using Plots: histogram
```

This article seeks to motivate Bijectors.jl and how distributions are transformed in the Turing.jl probabilistic programming language.

It assumes:

- some basic knowledge of probability distributions (the notions of sampling from them and calculating the probability density function for a given distribution); and
- some calculus (the chain and product rules for differentiation, and changes of variables in integrals).

## Sampling from a distribution

To sample from a distribution (as defined in [Distributions.jl](https://juliastats.org/Distributions.jl/)), we can use the `rand` function.
Let's sample from a normal distribution and then plot a histogram of the samples.

```{julia}
samples = rand(Normal(), 5000)
histogram(samples, bins=50)
```

(Calling `Normal()` without any arguments, as we do here, gives us a normal distribution with mean 0 and standard deviation 1.)
If you want to know the probability of observing any of the samples, you can use `logpdf`:

```{julia}
println("sample: $(samples[1])")
println("logpdf: $(logpdf(Normal(), samples[1]))")
```

The probability density function for the normal distribution with mean 0 and standard deviation 1 is

$$p(x) = \frac{1}{\sqrt{2\pi}} \exp{\left(-\frac{x^2}{2}\right)},$$

so we could also have calculated this manually using:

```{julia}
log(1 / sqrt(2π) * exp(-samples[1]^2 / 2))
```

## Sampling from a transformed distribution

Say that $x$ is distributed according to `Normal()`, and we want to draw samples of $y = \exp(x)$.
Now, $y$ is itself a random variable, and like any other random variable, will have a probability distribution, which we'll call $q(y)$.

In this specific case, the distribution of $y$ is known as a [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution).
For the purposes of this tutorial, let's implement our own `MyLogNormal` distribution that we can sample from.
(Distributions.jl already defines its own `LogNormal`, so we have to use a different name.)
To do this, we need to overload `Base.rand` for our new distribution.

```{julia}
struct MyLogNormal <: Distributions.ContinuousUnivariateDistribution
    μ::Float64
    σ::Float64
end
MyLogNormal() = MyLogNormal(0.0, 1.0)

function Base.rand(rng::Random.AbstractRNG, d::MyLogNormal)
  exp(rand(rng, Normal(d.μ, d.σ)))
end
```

Now we can do the same as above:

```{julia}
samples_lognormal = rand(MyLogNormal(), 5000)
# Cut off the tail for clearer visualisation
histogram(samples_lognormal, bins=0:0.1:5; xlims=(0, 5))
```

How do we implement `logpdf` for our new distribution, though?
Or in other words, if we observe a sample $y$, how do we know what the probability of drawing that sample was?

Naively, we might think to just un-transform the variable `y` by reversing the exponential, i.e. taking the logarithm.
We could then use the `logpdf` of the original distribution of `x`.

```{julia}
naive_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))
```

We can compare this function against the logpdf implemented in Distributions.jl:

```{julia}
println("Sample   : $(samples_lognormal[1])")
println("Expected : $(logpdf(LogNormal(), samples_lognormal[1]))")
println("Actual   : $(naive_logpdf(MyLogNormal(), samples_lognormal[1]))")
```

Clearly this approach is not quite correct!

## The derivative

The reason why this doesn't work is because transforming a (continuous) distribution causes probability density to be stretched and otherwise moved around.
For example, in the normal distribution, half of the probability density is between $-\infty$ and $0$, and half is between $0$ and $\infty$.
When exponentiated (i.e. in the log-normal distribution), the first half of the density is mapped to the interval $(0, 1)$, and the second half to $(1, \infty)$.

This 'explanation' on its own does not really mean much, though.
A perhaps more useful approach is to not talk about _probability densities_, but instead to make it more concrete by relating them to actual _probabilities_.
If we think about the normal distribution as a continuous curve, what the probability density function $p(x)$ really tells us is that: for any two points $a$ and $b$ (where $a \leq b$), the probability of drawing a sample between $a$ and $b$ is the corresponding area under the curve, i.e.

$$\int_a^b p(x) \, \mathrm{d}x.$$

For example, if $(a, b) = (-\infty, \infty)$, then the probability of drawing a sample from the entire distribution is 1.

Let's say that the probability density function of the log-normal distribution is $q(y)$.
Then, the area under the curve between the two points $\exp(a)$ and $\exp(b)$ is:

$$\int_{\exp(a)}^{\exp(b)} q(y) \, \mathrm{d}y.$$

This integral should be equal to the one above, because the probability of drawing from $[a, b]$ in the original distribution should be the same as the probability of drawing from $[\exp(a), \exp(b)]$ in the transformed distribution.
The question we have to solve here is: how do we find a function $q(y)$ such that this equality holds?

We can approach this by making the substitution $y = \exp(x)$ in the first integral (see [Wikipedia](https://en.wikipedia.org/wiki/Integration_by_substitution) for a refresher on substitutions in integrals, if needed).
We have that:

$$\frac{\mathrm{d}y}{\mathrm{d}x} = \exp(x) = y \implies \mathrm{d}x = \frac{1}{y}\,\mathrm{d}y$$

and so

$$\int_{x=a}^{x=b} p(x) \, \mathrm{d}x
  = \int_{y=\exp(a)}^{y=\exp(b)} p(\log(y)) \frac{1}{y} \,\mathrm{d}y
  = \int_{\exp(a)}^{\exp(b)} q(y) \, \mathrm{d}y,
$$

from which we can read off $q(y) = p(\log(y)) / y$.

In contrast, when we implemented `naive_logpdf`

```{julia}
naive_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))
```

that was the equivalent of saying that $q(y) = p(\log(y))$.
We left out a factor of $1/y$!

Indeed, now we can define the correct `logpdf` function.
Since everything is a logarithm here, instead of multiplying by $1/y$ we subtract $\log(y)$:

```{julia}
Distributions.logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y)) - log(y)
```

and check that it works:

```{julia}
println("Sample   : $(samples_lognormal[1])")
println("Expected : $(logpdf(LogNormal(), samples_lognormal[1]))")
println("Actual   : $(logpdf(MyLogNormal(), samples_lognormal[1]))")
```

The same process can be applied to _any_ kind of transformation.
If we have some transformation from $x$ to $y$, and the probability density functions of $x$ and $y$ are $p(x)$ and $q(y)$ respectively, then we have a general formula that:

$$q(y) = p(x) \left| \frac{\mathrm{d}x}{\mathrm{d}y} \right|.$$

In this case, we had $y = \exp(x)$, so $\mathrm{d}x/\mathrm{d}y = 1/y$.
(This equation is (11.5) in Bishop's textbook.)

::: {.callout-note}
The absolute value here takes care of the case where $f$ is a decreasing function, i.e., $f(x) > f(y)$ when $x < y$.
You can try this out with the transformation $y = -\exp(x)$.
If $a < b$, then $-\exp(a) > -\exp(b)$, and so you will have to swap the integration limits to ensure that the integral comes out positive.
:::

Note that $\mathrm{d}y/\mathrm{d}x$ is equal to $(\mathrm{d}x/\mathrm{d}y)^{-1}$, so the formula above can also be written as:

$$q(y) \left| \frac{\mathrm{d}y}{\mathrm{d}x} \right| = p(x).$$

## The Jacobian

In general, we may have transforms that act on multivariate distributions: for example, something mapping $p(x_1, x_2)$ to $q(y_1, y_2)$.
In this case, we need to extend the rule above by introducing what is known as the Jacobian matrix:

In this case, the rule above has to be extended by replacing the derivative $\mathrm{d}x/\mathrm{d}y$ with the determinant of the inverse Jacobian matrix:

$$\mathbf{J} = \begin{pmatrix}
\partial y_1/\partial x_1 & \partial y_1/\partial x_2 \\
\partial y_2/\partial x_1 & \partial y_2/\partial x_2
\end{pmatrix}.$$

This allows us to write the direct generalisation as:

$$q(y_1, y_2) \left| \det(\mathbf{J}) \right| = p(x_1, x_2),$$

or equivalently,

$$q(y_1, y_2) = p(x_1, x_2) \left| \det(\mathbf{J}^{-1}) \right|.$$

where $\mathbf{J}^{-1}$ is the inverse of the Jacobian matrix.
This is the same as equation (11.9) in Bishop.

::: {.callout-note}
Instead of inverting the original Jacobian matrix to get $\mathbf{J}^{-1}$, we could also use the Jacobian of the inverse function:

$$\mathbf{J}_\text{inv} = \begin{pmatrix}
\partial x_1/\partial y_1 & \partial x_1/\partial y_2 \\
\partial x_2/\partial y_1 & \partial x_2/\partial y_2
\end{pmatrix}.$$

As it turns out, these are entirely equivalent: the Jacobian of the inverse function is the inverse of the original Jacobian matrix.
:::

The rest of this section will be devoted to an example to show that this works, and contains some slightly less pretty mathematics.
If you are already suitably convinced by this stage, then you can skip the rest of this section.
(Or if you prefer something more formal, the Wikipedia article on integration by substitution [discusses the multivariate case as well](https://en.wikipedia.org/wiki/Integration_by_substitution#Substitution_for_multiple_variables).)

### An example: the Box–Muller transform

A motivating example where one might like to use a Jacobian is the [Box–Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform), which is a technique for sampling from a normal distribution.

The Box–Muller transform works by first sampling two random variables from the uniform distribution between 0 and 1:

$$\begin{align}
x_1 &\sim U(0, 1) \\
x_2 &\sim U(0, 1).
\end{align}$$

Both of these have a probability density function of $p(x) = 1$ for $0 < x \leq 1$, and 0 otherwise.
Because they are independent, we can write that

$$p(x_1, x_2) = p(x_1) p(x_2) = \begin{cases}
1 & \text{if } 0 < x_1 \leq 1 \text{ and } 0 < x_2 \leq 1, \\
0 & \text{otherwise}.
\end{cases}$$

The next step is to perform the transforms

$$\begin{align}
y_1 &= \sqrt{-2 \log(x_1)} \cos(2\pi x_2); \\
y_2 &= \sqrt{-2 \log(x_1)} \sin(2\pi x_2),
\end{align}$$

and it turns out that with these transforms, both $y_1$ and $y_2$ are independent and normally distributed with mean 0 and standard deviation 1, i.e.

$$q(y_1, y_2) = \frac{1}{2\pi} \exp{\left(-\frac{y_1^2}{2}\right)} \exp{\left(-\frac{y_2^2}{2}\right)}.$$

How can we show that this is the case?

There are many ways to work out the required calculus.
Some are more elegant and some rather less so!
One of the less headache-inducing ways is to define the intermediate variables:

$$r = \sqrt{-2 \log(x_1)}; \quad \theta = 2\pi x_2,$$

from which we can see that $y_1 = r\cos\theta$ and $y_2 = r\sin\theta$, and hence

$$\begin{align}
x_1 &= \exp{\left(-\frac{r^2}{2}\right)} = \exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)}; \\
x_2 &= \frac{\theta}{2\pi} = \frac{1}{2\pi} \, \arctan\left(\frac{y_2}{y_1}\right).
\end{align}$$

This lets us obtain the requisite partial derivatives in a way that doesn't involve _too_ much algebra.
As an example, we have

$$\frac{\partial x_1}{\partial y_1} = -y_1 \exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)} = -y_1 x_1,$$

(where we used the product rule), and

$$\frac{\partial x_2}{\partial y_1} = \frac{1}{2\pi} \left(\frac{1}{1 + (y_2/y_1)^2}\right) \left(-\frac{y_2}{y_1^2}\right),$$

(where we used the chain rule, and the derivative $\mathrm{d}(\arctan(a))/\mathrm{d}a = 1/(1 + a^2)$).

Putting together the Jacobian matrix, we have:

$$\mathcal{J} = \begin{pmatrix}
-y_1 x_1 & -y_2 x_1 \\
-cy_2/y_1^2 & c/y_1 \\
\end{pmatrix},$$

where $c = [2\pi(1 + (y_2/y_1)^2)]^{-1}$.
The determinant of this matrix is

$$\begin{align}
\det(\mathcal{J}) &= -cx_1 - cx_1(y_2/y_1)^2 \\
&= -cx_1\left[1 + \left(\frac{y_2}{y_1}\right)^2\right] \\
&= -\frac{1}{2\pi} x_1 \\
&= -\frac{1}{2\pi}\exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)},
\end{align}$$

Coming right back to our probability density, we have that

$$\begin{align}
q(y_1, y_2) &= p(x_1, x_2) \cdot |\det(\mathcal{J})| \\
&= \frac{1}{2\pi}\exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)},
\end{align}$$

as desired.

::: {.callout-note}
We haven't yet explicitly accounted for the fact that $p(x_1, x_2)$ is 0 if either $x_1$ or $x_2$ are outside the range $(0, 1]$.
For example, if this constraint on $x_1$ and $x_2$ were to result in inaccessible values of $y_1$ or $y_2$, then $q(y_1, y_2)$ should be 0 for those values.
Formally, for the transformation $f: X \to Y$ where $X$ is the unit square (i.e. $0 < x_1, x_2 \leq 1$), $q(y_1, y_2)$ should only take the above value for the [image](https://en.wikipedia.org/wiki/Image_(mathematics)) of $f$, and anywhere outside of the image it should be 0.

In our case, the $\log(x_1)$ term in the transform varies between 0 and $\infty$, and the $\cos(2\pi x_2)$ term ranges from $-1$ to $1$.
Hence $y_1$, which is the product of these two terms, ranges from $-\infty$ to $\infty$, and likewise for $y_2$.
So the image of $f$ is the entire real plane, and we don't have to worry about this.
:::


## Bijectors.jl

All the above has purely been a mathematical discussion of how distributions can be transformed.
Now, we turn to their implementation in Julia, specifically using the [Bijectors.jl package](https://github.com/TuringLang/Bijectors.jl).

A _bijection_ between two sets ([Wikipedia](https://en.wikipedia.org/wiki/Bijection)) is, essentially, a one-to-one mapping between the elements of these sets.
That is to say, if we have two sets $X$ and $Y$, then a bijection maps each element of $X$ to a unique element of $Y$.
To return to our univariate example, where we transformed $x$ to $y$ using $y = \exp(x)$, the exponentiation function is a bijection because every value of $x$ maps to one unique value of $y$.
The input set (the domain) is $(-\infty, \infty)$, and the output set (the codomain) is $(0, \infty)$.

Since bijections are a one-to-one mapping between elements, we can also reverse the direction of this mapping to create an inverse function. 
In the case of $y = \exp(x)$, the inverse function is $x = \log(y)$.

::: {.callout-note}
Technically, the bijections in Bijectors.jl are functions $f: X \to Y$ for which:

 - $f$ is continuously differentiable, i.e. the derivative $\mathrm{d}f(x)/\mathrm{d}x$ exists and is continuous (over the domain of interest $X$);
- If $f^{-1}: Y \to X$ is the inverse of $f$, then that is also continuously differentiable (over _its_ own domain, i.e. $Y$).

The technical mathematical term for this is a diffeomorphism ([Wikipedia](https://en.wikipedia.org/wiki/Diffeomorphism)), but we call them 'bijectors'.

When thinking about continuous differentiability, it's important to be conscious of the domains or codomains that we care about.
For example, taking the inverse function $\log(y)$ from above, its derivative is $1/y$, which is not continuous at $y = 0$.
However, we specified that the bijection $y = \exp(x)$ maps values of $x \in (-\infty, \infty)$ to $y \in (0, \infty)$, so the point $y = 0$ is not within the domain of the inverse function.
:::

Specifically, one of the primary purposes of Bijectors.jl is used to construct _bijections which map constrained distributions to unconstrained ones_.
For example, the log-normal distribution which we saw above is constrained: its _support_, i.e. the range over which $p(x) \geq 0$, is $(0, \infty)$.
However, we can transform that to an unconstrained distribution (the normal distribution) using the transformation $y = \log(x)$.

::: {.callout-note}
Bijectors.jl, as well as DynamicPPL (which we'll come to later), can work with a much broader class of bijective transformations of variables, not just ones that go to the entire real line.
But for the purposes of MCMC, unconstraining is the most common transformation, so we'll stick with that terminology.
:::


The `bijector` function, when applied to a distribution, returns a bijection $f$ that can be used to map the constrained distribution to an unconstrained one.
Unsurprisingly, for the log-normal distribution, the bijection is (a broadcasted version of) the $\log$ function.

```{julia}
import Bijectors as B

f = B.bijector(LogNormal())
```

We can apply this transformation to samples from the original distribution, for example:

```{julia}
samples_lognormal = rand(LogNormal(), 5)

samples_normal = f(samples_lognormal)
```

We can also obtain the inverse of a bijection, $f^{-1}$:

```{julia}
f_inv = B.inverse(f)

f_inv(samples_normal) == samples_lognormal
```

We know that the transformation $y = \log(x)$ changes the log-normal distribution to the normal distribution.
Bijectors.jl also gives us a way to access that transformed distribution:

```{julia}
transformed_dist = B.transformed(LogNormal(), f)
```

This type doesn't immediately look like a `Normal()`, but it behaves in exactly the same way.
For example, we can sample from it and plot a histogram:

```{julia}
samples_plot = rand(transformed_dist, 5000)
histogram(samples_plot, bins=50)
```

We can also obtain the logpdf of the transformed distribution and check that it is the same as that of a normal distribution:

```{julia}
println("Sample:   $(samples_plot[1])")
println("Expected: $(logpdf(Normal(), samples_plot[1]))")
println("Actual:   $(logpdf(transformed_dist, samples_plot[1]))")
```

Given the discussion in the previous sections, you might not be surprised to find that the logpdf of the transformed distribution is implemented using the Jacobian of the transformation.
In particular, it [directly uses](https://github.com/TuringLang/Bijectors.jl/blob/f52a9c52ede1f43155239447601387eb1dafe394/src/Bijectors.jl#L242-L255) the formula

$$\log(q(\mathbf{y})) = \log(p(\mathbf{x})) - \log(|\det(\mathbf{J})|).$$

You can access $\log(|\det(\mathbf{J})|)$ (evaluated at the point $\mathbf{x}$) using the `logabsdetjac` function:

```{julia}
# Reiterating the setup, just to be clear
x = rand(LogNormal())
f = B.bijector(LogNormal())
y = f(x)
transformed_dist = B.transformed(LogNormal(), f)

println("log(q(y))     : $(logpdf(transformed_dist, y))")
println("log(p(x))     : $(logpdf(LogNormal(), x))")
println("log(|det(J)|) : $(B.logabsdetjac(f, x))")
```

from which you can see that the equation above holds.
There are more functions available in the Bijectors.jl API; for full details do check out the [documentation](https://turinglang.org/Bijectors.jl/stable/).
For example, `logpdf_with_trans` can directly give us $\log(q(\mathbf{y}))$:

```{julia}
B.logpdf_with_trans(LogNormal(), x, true)
```

## The need for bijectors in MCMC

Constraints pose a problem for pretty much any kind of numerical method, and sampling is no exception to this.
The problem is that for any value $x$ outside of the support of a constrained distribution, $p(x)$ will be zero, and the logpdf will be $-\infty$.
Thus, any term that involves some ratio of probabilities (or equivalently, the logpdf)  will be infinite.

::: {.callout-note}
This post is already really long, and does not have quite enough space to explain either the Metropolis–Hastings or Hamiltonian Monte Carlo algorithms in detail.
If you need more information on these, please read e.g. chapter 11 of Bishop.
:::

### Metropolis–Hastings: fine?

This alone is not enough to cause issues for Metropolis–Hastings.
Here's an extremely barebones implementation of a random walk Metropolis algorithm:

```{julia}
# Take a step where the proposal is a normal distribution centred around
# the current value
function mh_step(p, x)
    x_proposed = rand(Normal(x, 1))
    acceptance_prob = min(1, p(x_proposed) / p(x))
    return if rand() < acceptance_prob
        x_proposed
    else
        x
    end
end

# Run a random walk Metropolis sampler.
# `p`  : a function that takes `x` and returns the pdf of the distribution
#        we're trying to sample from (up to a constant multiplicative factor)
# `x0` : the initial state
function mh(p, x0, n_samples)
    samples = []
    x = x0
    for _ in 2:n_samples
        x = mh_step(p, x)
        push!(samples, x)
    end
    return samples
end
```

With this we can sample from a log-normal distribution just fine:

```{julia}
p(x) = pdf(LogNormal(), x)
samples_with_mh = mh(p, 1.0, 5000)
histogram(samples_with_mh, bins=0:0.1:5; xlims=(0, 5))
```

In this MH implementation, the only place where $p(x)$ comes into play is in the acceptance probability.

As long as we make sure to start the sampling at a point within the support of the distribution, `p(x)` will be nonzero.
If the proposal step generates an `x_proposal` that is outside the support, `p(x_proposal)` will be zero, and the acceptance probability `p(x_proposal)/p(x)` will be zero.
So such a step will never be accepted, and the sampler will continue to stay within the support of the distribution.

Although this does mean that we may find ourselves having a higher reject rate than usual, and thus less efficient sampling, it at least does not cause the algorithm to become unstable or crash.

### Hamiltonian Monte Carlo: not so fine

The _real_ problem comes with gradient-based methods like Hamiltonian Monte Carlo (HMC).
Here's an equally barebones implementation of HMC.

```{julia}
using LinearAlgebra: I
import ForwardDiff

# Really basic leapfrog integrator.
# `z`        : position
# `r`        : momentum
# `timestep` : size of one integration step
# `nsteps`   : number of integration steps
# `dEdz`     : function that returns the derivative of the energy with respect
#              to `z`. The energy is the negative logpdf of the distribution
#              we're trying to sample from.
function leapfrog(z, r, timestep, nsteps, dEdz)
    function step_inner(z, r)
        # One small step for r, one giant leap for z
        r -= (timestep / 2) * dEdz(z)
        z += timestep * r
        # (and then one more small step for r)
        r -= (timestep / 2) * dEdz(z)
        return (z, r)
    end
    for _ in 1:nsteps
        z, r = step_inner(z, r)
    end
    (isnan(z) || isnan(r)) && error("Numerical instability encountered in leapfrog")
    return (z, -r)
end

# Take one HMC step.
# `z` : current position
# `E` : function that returns the energy (negative logpdf) at `z`
# Other arguments are as above
function hmc_step(z, E, dEdz, integ_timestep, integ_nsteps)
    # Generate new momentum
    r = randn()
    # Integrate the Hamiltonian dynamics
    z_new, r_new = leapfrog(z, r, integ_timestep, integ_nsteps, dEdz)
    # Calculate Hamiltonian
    H = E(z) + 0.5 * sum(r .^ 2)
    H_new = E(z_new) + 0.5 * sum(r_new .^ 2)
    # Acceptance criterion
    accept_prob = min(1, exp(H - H_new))
    return if rand() < accept_prob
        z_new
    else
        z
    end
end

# Run HMC.
# `z0` : initial position
# Other arguments are as above
function hmc(z0, E, dEdz, nsteps; integ_timestep=0.1, integ_nsteps=100)
    samples = [z0]
    z = z0
    for _ in 2:nsteps
        z = hmc_step(z, E, dEdz, integ_timestep, integ_nsteps)
        push!(samples, z)
    end
    return samples
end
```

Okay, that's our HMC set up.
Now, let's try to sample from a log-normal distribution:

```{julia}
#| error: true
p(x) = pdf(LogNormal(), x)
E(x) = -log(p(x))
dEdz(x) = ForwardDiff.derivative(E, x)
samples_with_hmc = hmc(1.0, E, dEdz, 5000)
histogram(samples_with_hmc, bins=0:0.1:5; xlims=(0, 5))
```

Eeeek! What happened?
It turns out that evaluating the gradient of the energy at any point outside the support of the distribution is not possible:

```{julia}
dEdz(-1)
```

This is because $p(x)$ is 0, and hence $E(x) = -\log(p(x))$ is $\infty$ outside the support.
If we try to evaluate the gradient at such a point, it's simply undefined, because arithmetic on infinity doesn't make sense:

```{julia}
Inf - Inf
```

To really pinpoint where this is happening, we need to look into the HMC leapfrog integration, specifically these lines:

```julia
r -= (timestep / 2) * dEdz(z)   #  (1)
z += timestep * r               #  (2)
r -= (timestep / 2) * dEdz(z)   #  (3)
```

Here, `z` is the position and `r` the momentum.
Since we start our sampler inside the support of the distribution (by supplying a good initial point), `dEdz(z)` will start off being well-defined on line (1).
However, after `r` is updated on line (1), `z` is updated again on line (2), and _this_ value of `z` may well be outside of the support.
At this point, `dEdz(z)` will be `NaN`, and the final update to `r` on line (3) will also cause it to be `NaN`.

Even if we're lucky enough for an individual integration step to not move `z` outside the support, there are many integration steps per sampler step, and many sampler steps, and so the chances of this happening at some point are quite high.

It's possible to choose our integration parameters carefully to reduce the risk of this happening.
For example, we could set the integration timestep to be _really_ small, thus reducing the chance of making a move outside the support.
But that will just lead to a very slow exploration of parameter space, and in general, we should like to avoid this problem altogether.

### Rescuing HMC

Perhaps unsurprisingly, the answer to this is to transform the underlying distribution to an unconstrained one and sample from that instead.
However, we have to make sure that we include the pesky Jacobian term when sampling from the transformed distribution.
That's where Bijectors.jl can come in.

The main change we need to make is to pass a modified probability density to our HMC sampler.
Recall that

$$\log(q(\mathbf{y})) = \log(p(\mathbf{x})) - \log(|\det(\mathbf{J})|).$$

Instead of passing $E = -\log(p(\mathbf{x})$, we will instead pass $E = -\log(q(\mathbf{y}))$, meaning that the HMC sampler sees the transformed distribution $q(y)$ rather than the original distribution $p(x)$.
Conveniently, Bijectors.jl lets us calculate $\log(q(y))$ using `logpdf_with_trans(p, x, true)`:

```{julia}
d = LogNormal()
f = B.bijector(d)     # Transformation function
f_inv = B.inverse(f)  # Inverse transformation function

function logq(y)
    x = f_inv(y)
    return B.logpdf_with_trans(d, x, true)
end
E(z) = -logq(z)
dEdz(z) = ForwardDiff.derivative(E, z)
```

Now, because our transformed distribution is unconstrained, we can evaluate `E` and `dEdz` at any point, and sample with more confidence:

```{julia}
samples_with_hmc = hmc(1.0, E, dEdz, 5000)
samples_with_hmc[1:5]
```

There aren't any sampling errors this time, thankfully!
We have to remember that when we run HMC on this, it will give us back samples of `y`, not `x`.
So we can untransform them:

```{julia}
samples_with_hmc_untransformed = f_inv(samples_with_hmc)
histogram(samples_with_hmc_untransformed, bins=0:0.1:5; xlims=(0, 5))
```

How do we know that this has sampled correctly?
For one, we can check that the mean of the samples are what we expect them to be.
From [Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution), the mean of a log-normal distribution is given by $\exp[\mu + (\sigma^2/2)]$.
For our log-normal distribution, we set $\mu = 0$ and $\sigma = 1$, so:

```{julia}
println("expected mean: $(exp(0 + (1^2/2)))")
println("  actual mean: $(mean(samples_with_hmc_untransformed))")
```

The importance of the Jacobian term here isn't to enable sampling _per se_.
Because the resulting distribution is unconstrained, we could have still sampled from it without using the Jacobian.
However, adding the Jacobian is what ensures that when we un-transform the samples, we get the correct distribution.

The next code block shows what happens if we don't include the Jacobian term.
In this `logq_wrong`, we've un-transformed `y` to `x` and calculated the logpdf with respect to its original distribution.
This is exactly the same mistake that we made at the start of this article with `naive_logpdf`.

```{julia}
function logq_wrong(y)
    x = f_inv(y)
    return logpdf(d, x)
end
E(z) = -logq_wrong(z)
dEdz(z) = ForwardDiff.derivative(E, z)
samples_questionable = hmc(1.0, E, dEdz, 5000)
samples_questionable_untransformed = f_inv(samples_questionable)

println("mean: $(mean(samples_questionable_untransformed))")
```

You can see that even though the sampling ran fine without errors, the mean is completely wrong.

## Linked and unlinked VarInfos in DynamicPPL

In the final section of this article, we'll discuss the higher-level implications of constrained distributions in the Turing.jl framework.

When we are performing Bayesian inference, we're trying to sample from a joint probability distribution, which isn't usually a single, well-defined distribution like in the rather simplified example above.
However, each random variable in the model will have its own distribution, and often some of these will be constrained.
For example, if `b ~ LogNormal()` is a random variable in a model, then $p(b)$ will be zero for any $b \leq 0$.
Consequently, any joint probability $p(b, c, \ldots)$ will also be zero for any combination of parameters where $b \leq 0$, and so that joint distribution is itself constrained.

To get around this, DynamicPPL allows the variables to be transformed in exactly the same way as above.
For simplicity, consider the following model:

```{julia}
using DynamicPPL

@model function demo()
    x ~ LogNormal()
end

model = demo()
vi = VarInfo(model)
vn_x = @varname(x)
# Retrieve the 'internal' representation of x – we'll explain this later
DynamicPPL.getindex_internal(vi, vn_x)
```

The call to `VarInfo` executes the model once and stores the sampled value inside `vi`.
By default, `VarInfo` itself stores un-transformed values.
We can see this by comparing the value of the logpdf stored inside the `VarInfo`:

```{julia}
DynamicPPL.getlogp(vi)
```

with a manual calculation:

```{julia}
logpdf(LogNormal(), DynamicPPL.getindex_internal(vi, vn_x))
```

In DynamicPPL, the `link` function can be used to transform the variables.
This function does three things: firstly, it transforms the variables; secondly, it updates the value of logp (by adding the Jacobian term); and thirdly, it sets a flag on the variables to indicate that it has been transformed.
Note that this acts on _all_ variables in the model, including unconstrained ones.
(Unconstrained variables just have an identity transformation.)

```{julia}
vi_linked = DynamicPPL.link(vi, model)
println("Transformed value: $(DynamicPPL.getindex_internal(vi_linked, vn_x))")
println("Transformed logp: $(DynamicPPL.getlogp(vi_linked))")
println("Transformed flag: $(DynamicPPL.istrans(vi_linked, vn_x))")
```

Indeed, we can see that the new logp value matches with

```{julia}
logpdf(Normal(), DynamicPPL.getindex_internal(vi_linked, vn_x))
```

The reverse transformation, `invlink`, reverts all of the above steps:

```{julia}
vi = DynamicPPL.invlink(vi_linked, model)  # Same as the previous vi
println("Un-transformed value: $(DynamicPPL.getindex_internal(vi, vn_x))")
println("Un-transformed logp: $(DynamicPPL.getlogp(vi))")
println("Un-transformed flag: $(DynamicPPL.istrans(vi, vn_x))")
```

### Model and internal representations

In DynamicPPL, there is a difference between the value of a random variable and its 'internal' value.
This is most easily seen by first transforming, and then comparing the output of `getindex` and `getindex_internal`.
The former extracts the regular value, which we call the **model representation** (because it is consistent with the distribution specified in the model).
The latter, as the name suggests, gets the **internal representation** of the variable, which is how it is actually stored in the VarInfo object.

```{julia}
println("   Model representation: $(getindex(vi_linked, vn_x))")
println("Internal representation: $(DynamicPPL.getindex_internal(vi_linked, vn_x))")
```

::: {.callout-note}
Note that `vi_linked[vn_x]` can also be used as shorthand for `getindex(vi_linked, vn_x)`; this usage is common in the DynamicPPL/Turing codebase.
:::

We can see (for this linked varinfo) that there are _two_ differences between these outputs:

1. _The internal representation has been transformed using the bijector (in this case, the log function)._
   This means that the `istrans()` flag which we used above doesn't modify the model representation: it only tells us whether the internal representation has been transformed or not.

2. _The internal representation is a vector, whereas the model representation is a scalar._
   This is because in DynamicPPL, _all_ internal values are vectorised (i.e. converted into some vector), regardless of distribution. On the other hand, since the model specifies a univariate distribution, the model representation is a scalar.

One might also ask, what is the internal representation for an _unlinked_ varinfo?

```{julia}
println("   Model representation: $(getindex(vi, vn_x))")
println("Internal representation: $(DynamicPPL.getindex_internal(vi, vn_x))")
```

For an unlinked VarInfo, the internal representation is vectorised, but not transformed.
We call this an **unlinked internal representation**; conversely, when the VarInfo has been linked, each variable will have a corresponding **linked internal representation**.

This sequence of events is summed up in the following diagram, where `f(..., args)` indicates that the `...` is to be replaced with the object at the beginning of the arrow:

![Functions related to variable transforms in DynamicPPL](./dynamicppl_link.png)

In the final part of this article, we'll take a more in-depth look at the internal DynamicPPL machinery that allows us to convert between representations and obtain the correct probability densities.
Before that, though, we'll take a quick high-level look at how the HMC sampler in Turing.jl uses the functions introduced so far.

## Case study: HMC in Turing.jl

While DynamicPPL provides the _functionality_ for transforming variables, the transformation itself happens at an even higher level, i.e. in the sampler itself.
The HMC sampler in Turing.jl is in [this file](https://github.com/TuringLang/Turing.jl/blob/5b24cebe773922e0f3d5c4cb7f53162eb758b04d/src/mcmc/hmc.jl).
In the first step of sampling, it calls `link` on the sampler.
This transformation is preserved throughout the sampling process, meaning that `istrans()` always returns true.

We can observe this by inserting print statements into the model.
Here, `__varinfo__` is the internal symbol for the `VarInfo` object used in model evaluation:

```{julia}
using Turing
setprogress!(false)

@model function demo2()
    x ~ LogNormal()
    if x isa AbstractFloat
        println("-----------")
        println("model repn: $(DynamicPPL.getindex(__varinfo__, @varname(x)))")
        println("internal repn: $(DynamicPPL.getindex_internal(__varinfo__, @varname(x)))")
        println("istrans: $(istrans(__varinfo__, @varname(x)))")
    end
end

sample(demo2(), HMC(0.1, 3), 3);
```

(Here, the check on `if x isa AbstractFloat` prevents the printing from occurring during computation of the derivative.)
You can see that during the three sampling steps, `istrans` is always kept as `true`.

::: {.callout-note}
The first two model evaluations where `istrans` is `false` occur prior to the actual sampling.
One occurs when the model is checked for correctness (using [`DynamicPPL.check_model_and_trace`](https://github.com/TuringLang/DynamicPPL.jl/blob/ba490bf362653e1aaefe298364fe3379b60660d3/src/debug_utils.jl#L582-L612)).
The second occurs because the model is evaluated once to generate a set of initial parameters inside [DynamicPPL's implementation of `AbstractMCMC.step`](https://github.com/TuringLang/DynamicPPL.jl/blob/ba490bf362653e1aaefe298364fe3379b60660d3/src/sampler.jl#L98-L117).
Both of these steps occur with all samplers in Turing.jl, so are not specific to the HMC example shown here.
:::

What this means is that from the perspective of the HMC sampler, it _never_ sees the constrained variable: it always thinks that it is sampling from an unconstrained distribution.

The biggest prerequisite for this to work correctly is that the potential energy term in the Hamiltonian—or in other words, the model log density—must be programmed correctly to include the Jacobian term.
This is exactly the same as how we had to make sure to define `logq(y)` correctly in the toy HMC example above.

Within Turing.jl, this is correctly handled because a statement like `x ~ LogNormal()` in the model definition above is translated into `assume(LogNormal(), @varname(x), __varinfo__)`, defined [here](https://github.com/TuringLang/DynamicPPL.jl/blob/ba490bf362653e1aaefe298364fe3379b60660d3/src/context_implementations.jl#L225-L229).
If you follow the trail of function calls, you can verify that the `assume` function does indeed check for the presence of the `istrans` flag and adds the Jacobian term accordingly.

## A deeper dive into DynamicPPL's internal machinery

As described above, DynamicPPL stores a (possibly linked) _internal representation_ which is accessible via `getindex_internal`, but can also provide users with the original, untransformed, _model representation_ via `getindex`.
This abstraction allows the user to obtain samples from constrained distributions without having to perform the transformation themselves.

![More functions related to variable transforms in DynamicPPL](./dynamicppl_link2.png)

The conversion between these representations is done using several internal functions in DynamicPPL, as depicted in the above diagram.
The following operations are labelled:

1. This is linking, i.e. transforming a constrained variable to an unconstrained one.

2. This is vectorisation: for example, converting a scalar value to a 1-element vector.

3. This arrow brings us from the model representation to the linked internal representation.
   This is the composition of (1) and (2): linking and then vectorising.

4. This arrow brings us from the model representation to the unlinked internal representation.
   This only requires a single step, vectorisation.

Each of these steps can be accomplished using the following functions.

|     | To get the function | To get the inverse function |
| --- | ------------------- | --------------------------- |
| (1) | `link_transform(dist)` | `invlink_transform(dist)` |
| (2) | `to_vec_transform(dist)` | `from_vec_transform(dist)` |
| (3) | `to_linked_internal_transform(vi, vn[, dist])` | `from_linked_internal_transform(vi, vn[, dist])` |
| (4) | `to_internal_transform(vi, vn[, dist])` | `from_internal_transform(vi, vn[, dist])` |

Note that these functions do not perform the actual transformation; rather, they return the transformation function itself.
For example, let's take a look at the `VarInfo` from the previous section, which contains a single variable `x ~ LogNormal()`.

```{julia}
model_repn = vi[vn_x]
```

```{julia}
# (1) Get the link function
f_link = DynamicPPL.link_transform(LogNormal())
# (2) Get the vectorise function
f_vec = DynamicPPL.to_vec_transform(LogNormal())

# Apply it to the model representation
linked_internal_repn = f_vec(f_link(model_repn))
```

Equivalently, we could have done:

```{julia}
# (3) Get the linked internal transform function
f_linked_internal = DynamicPPL.to_linked_internal_transform(vi, vn_x, LogNormal())

# Apply it to the model representation
linked_internal_repn = f_linked_internal(model_repn)
```

And let's confirm that this is the same as the linked internal representation, using the `VarInfo` that we linked earlier:

```{julia}
DynamicPPL.getindex_internal(vi_linked, vn_x)
```

The purpose of having all of these machinery is to allow other parts of DynamicPPL, such as the tilde pipeline, to handle transformed variables correctly.
The following diagram shows how `assume` first checks whether the variable is transformed (using `istrans`), and then applies the appropriate transformation function.

<!-- 'wrappingWidth' setting required because of https://github.com/mermaid-js/mermaid-cli/issues/112#issuecomment-2352670995 -->
```{mermaid}
%%{ init: { 'themeVariables': { 'lineColor': '#000000' } } }%%
%%{ init: { 'flowchart': { 'curve': 'linear', 'wrappingWidth': -1 } } }%%
graph TD
    A["x ~ LogNormal()"]:::boxStyle
    B["vn = <span style='color:#3B6EA8 !important;'>@varname</span>(x)<br>dist = LogNormal()<br>x, vi = ..."]:::boxStyle
    C["assume(vn, dist, vi)"]:::boxStyle
    D(["<span style='color:#3B6EA8 !important;'>if</span> istrans(vi, vn)"]):::boxStyle
    E["f = from_internal_transform(vi, vn, dist)"]:::boxStyle
    F["f = from_linked_internal_transform(vi, vn, dist)"]:::boxStyle
    G["x, logjac = with_logabsdet_jacobian(f, getindex_internal(vi, vn, dist))"]:::boxStyle
    H["<span style='color:#3B6EA8 !important;'>return</span> x, logpdf(dist, x) - logjac, vi"]:::boxStyle
    
    A -.->|<span style='color:#3B6EA8 ; background-color:#ffffff;'>@model</span>| B
    B -.->|<span style='color:#000000 ; background-color:#ffffff;'>tilde-pipeline</span>| C
    C --> D
    D -->|<span style='color:#97365B ; background-color:#ffffff;'>false</span>| E
    D -->|<span style='color:#97365B ; background-color:#ffffff;'>true</span>| F
    E --> G
    F --> G
    G --> H

    classDef boxStyle fill:#ffffff,stroke:#000000,font-family:Courier,color:#000000
    linkStyle default stroke:#000000,stroke-width:1px,color:#000000
```

Here, `with_logabsdet_jacobian` is defined [in the ChangesOfVariables.jl package](https://juliamath.github.io/ChangesOfVariables.jl/stable/api/#ChangesOfVariables.with_logabsdet_jacobian), and returns both the effect of the transformation `f` as well as the log Jacobian term.

Because we chose `f` appropriately, we find here that `x` is always the model representation; furthermore, if the variable was _not_ linked (i.e. `istrans` was false), the log Jacobian term will be zero.
However, if it was linked, then the Jacobian term would be appropriately included, making sure that sampling proceeds correctly.

## Why do we need to do this at runtime?

Given that we know whether a `VarInfo` is linked or not, one might wonder why we need both `from_internal_transform` and `from_linked_internal_transform` at the point where the model is evaluated.
Could we not, for example, store the required transformation inside the `VarInfo` when we link it, and simply reuse that each time?

That is, why can't we just do

```{mermaid}
%%{ init: { 'flowchart': { 'curve': 'linear', 'wrappingWidth': -1 } } }%%
%%{ init: { 'themeVariables': { 'lineColor': '#000000' } } }%%
graph TD
      A["assume(varinfo, <span style='color:#3B6EA8 !important;'>@varname</span>(x), Normal())"]:::boxStyle
      B["f = from_internal_transform(varinfo, varname, dist)"]:::boxStyle
      C["x, logjac = with_logabsdet_jacobian(f, getindex_internal(varinfo, varname))"]:::boxStyle
      D["<span style='color:#3B6EA8 !important;'>return</span> x, logpdf(dist, x) - logjac, varinfo"]:::dashedBox
      
      A --> B
      B --> C
      C --> D

    classDef dashedBox fill:#ffffff,stroke:#000000,stroke-dasharray: 5 5,font-family:Courier,color:#000000
    classDef boxStyle fill:#ffffff,stroke:#000000,font-family:Courier,color:#000000

    linkStyle default stroke:#000000,stroke-width:1px,color:#000000
```

where `from_internal_transform` here only looks up a stored transformation function?

Unfortunately, this is not possible in general, because the transformation function might not be a constant between different model evaluations.
Consider, for example, the following model:

```{julia}
@model function demo_dynamic_constraint()
    m ~ Normal()
    x ~ truncated(Normal(); lower=m)
    return (m=m, x=x)
end
```

Here, `m` is distributed according to a plain `Normal()`, whereas the variable `x` is constrained to be in the domain `(m, Inf)`.
Because of this, we expect that any time we sample from the model, we should have that `m < x` (in terms of their model representations):

```{julia}
model = demo_dynamic_constraint()
vi = VarInfo(model)
vn_m, vn_x = @varname(m), @varname(x)

vi[vn_m], vi[vn_x]
```

(Note that `vi[vn]` is a shorthand for `getindex(vi, vn)`, so this retrieves the model representations of `m` and `x`.)
So far, so good.
Let's now link this `VarInfo` so that we end up working in an 'unconstrained' space, where both `m` and `x` can take on any values in `(-Inf, Inf)`.
First, we should check that the model representations are unchanged when linking:

```{julia}
vi_linked = link(vi, model)
vi_linked[vn_m], vi_linked[vn_x]
```

But if we change the value of `m`, to, say, a bit larger than `x`:

```{julia}
# Update the model representation for `m` in `vi_linked`.
vi_linked[vn_m] = vi_linked[vn_x] + 1
vi_linked[vn_m], vi_linked[vn_x]
```

::: {.callout-warning}
This is just for demonstration purposes!
You shouldn't be directly setting variables in a linked `varinfo` like this unless you know for a fact that the value will be compatible with the constraints of the model.
:::

Now, we see that the constraint `m < x` is no longer satisfied.
Hence, one might expect that if we try to evaluate the model using this `VarInfo`, we should obtain an error.
Here, `evaluate!!` returns two things: the model's return value itself (which we defined above to be a `NamedTuple`), and the resulting `VarInfo` post-evaluation.

```{julia}
retval, ret_varinfo = DynamicPPL.evaluate!!(model, vi_linked, DefaultContext())
getlogp(ret_varinfo)
```

But we don't get any errors!
Indeed, we could even calculate the 'log probability density' for this evaluation.

To understand this, we need to look at the actual value which was used during the model evaluation.
We can glean this from the return value (or from the returned `VarInfo`, but the former is easier):

```{julia}
retval
```

We can see here that the model evaluation used the value of `m` that we provided, but the value of `x` was 'updated'.

The reason for this is that internally in a model evaluation, we construct the transformation function from the internal to the model representation based on the *current* realizations in the model!
That is, we take the `dist` in a `x ~ dist` expression _at model evaluation time_ and use that to construct the transformation, thus allowing it to change between model evaluations without invalidating the transformation.

Knowing that the distribution of `x` depends on the value of `m`, we can now understand how the model representation of `x` got updated.
The linked `VarInfo` does not store the model representation of `x`, but only its linked internal representation.
So, what happened during the model evaluation above was that the linked internal representation of `x` – which was constructed using the _original_ value of `m` – was transformed back into a new model representation using a _different_ value of `m`.

We can reproduce the 'new' value of `x` by performing these transformations manually:

```{julia}
# Generate a fresh linked VarInfo (without the new / 'corrupted' values)
vi_linked = link(vi, model)
# See the linked internal representations
DynamicPPL.getindex_internal(vi_linked, vn_m), DynamicPPL.getindex_internal(vi_linked, vn_x)
```

Now we update the value of `m` like we did before:

```{julia}
vi_linked[vn_m] = vi_linked[vn_x] + 1
vi_linked[vn_m]
```

When evaluating the model, the distribution of `x` is now changed, and so is the corresponding inverse bijector:

```{julia}
new_dist_x = truncated(Normal(); lower=vi_linked[vn_m])
new_f_inv = DynamicPPL.invlink_transform(new_dist_x)
```

and if we apply this to the internal representation of `x`:

```{julia}
new_f_inv(DynamicPPL.getindex_internal(vi_linked, vn_x))
```
