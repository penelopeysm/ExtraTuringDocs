---
title: "Transforms and distributions"
engine: julia
---

```{julia}
import Random
Random.seed!(468);
```

This article seeks to motivate Bijectors.jl and how distributions are transformed in the Turing.jl probabilistic programming language.

It assumes:

- some basic knowledge of probability distributions (the notions of sampling from them and calculating the probability density function for a given distribution); and
- some calculus (the chain and product rules for differentiation, and changes of variables in integrals).

## Sampling from a distribution

To sample from a distribution (as defined in Distributions.jl), we can use the `rand` function.
Let's sample from a normal distribution and then plot a histogram of the samples.
Calling `Normal()` with no arguments gives a standard normal distribution with mean 0 and standard deviation 1.

```{julia}
using Distributions
using Plots

samples = rand(Normal(), 5000)
histogram(samples, bins=50)
```

That's all great, and furthermore if you want to know the probability of observing any of the samples, you can use `logpdf`:

```{julia}
(samples[1], logpdf(Normal(), samples[1]))
```

The probability density function for the normal distribution with mean 0 and standard deviation 1 is

$$p(x) = \frac{1}{\sqrt{2\pi}} \exp{\left(-\frac{x^2}{2}\right)},$$

so we could also have calculated this manually using:

```{julia}
log(1 / sqrt(2π) * exp(-samples[1]^2 / 2))
```

## Sampling from a transformed distribution

Say that $x$ is distributed according to `Normal()`, and we want to draw samples of $y = \exp(x)$.
Now, $y$ is itself a random variable, and like any other random variable, will have a probability distribution, which we'll call $q(y)$.

In this specific case, the distribution of $y$ is known as a [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution).
For illustration purposes, let's try to implement our own `MyLogNormal` distribution that we can sample from.
(Distributions.jl already defines its own `LogNormal`, so we have to use a different name.)

```{julia}
struct MyLogNormal <: ContinuousUnivariateDistribution
    μ::Float64
    σ::Float64
end
MyLogNormal() = MyLogNormal(0.0, 1.0)

Base.rand(rng::Random.AbstractRNG, d::MyLogNormal) = exp(rand(rng, Normal(d.μ, d.σ)))
```

Now we can do the same as above:

```{julia}
samples_lognormal = rand(MyLogNormal(), 5000)
# Cut off the tail for clearer visualisation
histogram(samples_lognormal, bins=0:0.1:5; xlims=(0, 5))
```

How do we implement `logpdf` for our new distribution, though?
Or in other words, if we observe a sample $y$, how do we know what the probability of drawing that sample was?

Naively, we might think to just un-transform the variable `y` by reversing the exponential, i.e. taking the logarithm
We could then use the `logpdf` of the original distribution of `x`.

```{julia}
naive_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))
```

We can compare this function against the logpdf implemented in Distributions.jl:

```{julia}
println("Sample   : $(samples_lognormal[1])")
println("Expected : $(logpdf(LogNormal(), samples_lognormal[1]))")
println("Actual   : $(naive_logpdf(MyLogNormal(), samples_lognormal[1]))")
```

Clearly this approach is not quite correct!

## The derivative

Fundamentally, the reason why this doesn't work is because transforming a (continuous) distribution causes probability density to be stretched and otherwise moved around.

::: {.callout-note}
There are various posts on the Internet that explain this visually.
:::

A perhaps useful approach is to not talk about _probability densities_, but instead to make it more concrete by talking about actual _probabilities_.
If we think about the normal distribution as a continuous curve, what the probability density function $p(x)$ really tells us is that: for any two points $a$ and $b$ (where $a \leq b$), the probability of drawing a sample between $a$ and $b$ is the corresponding area under the curve, i.e.

$$\int_a^b p(x) \, \mathrm{d}x.$$

For example, if $(a, b) = (-\infty, \infty)$, then the probability of drawing a sample from the entire distribution is 1.

Let's say that the probability density function of the log-normal distribution is $q(y)$.
Then, the area under the curve between the two points $\exp(a)$ and $\exp(b)$ is:

$$\int_{\exp(a)}^{\exp(b)} q(y) \, \mathrm{d}y.$$

This integral should be equal to the one above, because the probability of drawing from $[a, b]$ in the original distribution should be the same as the probability of drawing from $[\exp(a), \exp(b)]$ in the transformed distribution.
The question we have to solve here is: how do we find a function $q(y)$ such that this equality holds?

We can approach this by substituting $y = \exp(x)$ into the first integral (see [Wikipedia](https://en.wikipedia.org/wiki/Integration_by_substitution) for a refresher if needed).
We have that:

$$\frac{\mathrm{d}y}{\mathrm{d}x} = \exp(x) = y \implies \mathrm{d}x = \frac{1}{y}\,\mathrm{d}y$$

and so

$$\int_{x=a}^{x=b} p(x) \, \mathrm{d}x
  \longrightarrow \int_{y=\exp(a)}^{y=\exp(b)} \underbrace{p(\log(y)) \frac{1}{y}}_{q(y)} \,\mathrm{d}y,$$

from which we can read off $q(y) = p(\log(y)) / y$.

In contrast, when we implemented `naive_logpdf`

```{julia}
naive_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))
```

that was the equivalent of saying that $q(y) = p(\log(y))$.
We left out a factor of $1/y$!

Indeed, now we can define the correct `logpdf` function.
Since everything is a logarithm here, instead of multiplying by $1/y$ we subtract $\log(y)$:

```{julia}
Distributions.logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y)) - log(y)
```

and check that it works:

```{julia}
println("Sample   : $(samples_lognormal[1])")
println("Expected : $(logpdf(LogNormal(), samples_lognormal[1]))")
println("Actual   : $(logpdf(MyLogNormal(), samples_lognormal[1]))")
```

The same logic can be applied to _any_ kind of transformation.
If we have some transformation from $x$ to $y$, and the probability density functions of $x$ and $y$ are $p(x)$ and $q(y)$ respectively, then

$$q(y) = p(x) \left| \frac{\mathrm{d}x}{\mathrm{d}y} \right|.$$

In this case, we had $y = \exp(x)$, so $\mathrm{d}x/\mathrm{d}y = 1/y$.
This equation is (11.5) in Bishop's textbook.

::: {.callout-note}
The absolute value here takes care of the case where $f$ is decreasing, i.e., the distribution is flipped.
You can try this out with the transformation $y = -\exp(x)$.
If $a < b$, then $-exp(a) > -exp(b)$, and so you will have to swap the integration limits to ensure that the integral comes out positive.
:::

## The Jacobian

In general, we may have transforms that act on multivariate distributions, for example something mapping $p(x_1, x_2)$ to $q(y_1, y_2)$.
In this case, the rule above has to be extended by replacing the derivative $\mathrm{d}x/\mathrm{d}y$ with the determinant of the Jacobian matrix:

$$\mathcal{J} = \begin{pmatrix}
\partial x_1/\partial y_1 & \partial x_1/\partial y_2 \\
\partial x_2/\partial y_1 & \partial x_2/\partial y_2
\end{pmatrix}.$$

and specifically,

$$q(y_1, y_2) = p(x_1, x_2) \left| \det(\mathcal{J}) \right|.$$

This is the same as equation (11.9) in Bishop, except that he denotes the absolute value of the determinant with just $|\mathcal{J}|$.

::: {.callout-important}
Note that, if we have a function $f$ mapping $\mathbf{x}$ to $\mathbf{y}$, then the Jacobian matrix $\mathbf{J}$ (sometimes denoted $\mathbf{J}_f$) is usually defined _the other way round_:

$$\mathbf{J} = \begin{pmatrix}
\partial y_1/\partial x_1 & \partial y_1/\partial x_2 \\
\partial y_2/\partial x_1 & \partial y_2/\partial x_2
\end{pmatrix}.$$

Indeed, later in this article we will see that Bijectors.jl uses this convention.
This is why we have denoted this 'inverse' Jacobian as $\mathcal{J}$, rather than $\mathbf{J}$.

$\mathcal{J}$ is really the Jacobian of the inverse function $f^{-1}$.
As it turns out, the matrix $\mathcal{J}$ is also the inverse of $\mathbf{J}$.
:::

The rest of this section will be devoted to an example to show that this works, and contains some slightly less pretty mathematics.
If you are already suitably convinced by this stage, then you can skip the rest of this section.
(Or if you prefer something more formal, the Wikipedia article on integration by substitution [discusses the multivariate case as well](https://en.wikipedia.org/wiki/Integration_by_substitution#Substitution_for_multiple_variables).)

### An example: the Box–Muller transform

A motivating example where one might like to use a Jacobian is the [Box–Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform), which is a technique for sampling from a normal distribution.

The Box–Muller transform works by first sampling two random variables from the uniform distribution between 0 and 1:

$$\begin{align}
x_1 &\sim U(0, 1) \\
x_2 &\sim U(0, 1).
\end{align}$$

Both of these have a probability density function of $p(x) = 1$ for $0 < x \leq 1$, and 0 otherwise.
Because they are independent, we can write that

$$p(x_1, x_2) = p(x_1) p(x_2) = \begin{cases}
1 & \text{if } 0 < x_1 \leq 1 \text{ and } 0 < x_2 \leq 1, \\
0 & \text{otherwise}.
\end{cases}$$

The next step is to perform the transforms

$$\begin{align}
y_1 &= \sqrt{-2 \log(x_1)} \cos(2\pi x_2); \\
y_2 &= \sqrt{-2 \log(x_1)} \sin(2\pi x_2),
\end{align}$$

and it turns out that with these transforms, both $y_1$ and $y_2$ are independent and normally distributed with mean 0 and standard deviation 1, i.e.

$$q(y_1, y_2) = \frac{1}{2\pi} \exp{\left(-\frac{y_1^2}{2}\right)} \exp{\left(-\frac{y_2^2}{2}\right)}.$$

How can we show that this is the case?

There are many ways to work out the required calculus.
Some are more elegant and some rather less so!
One of the less headache-inducing ways is to define the intermediate variables:

$$r = \sqrt{-2 \log(x_1)}; \quad \theta = 2\pi x_2,$$

from which we can see that $y_1 = r\cos\theta$ and $y_2 = r\sin\theta$, and hence

$$\begin{align}
x_1 &= \exp{\left(-\frac{r^2}{2}\right)} = \exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)}; \\
x_2 &= \frac{\theta}{2\pi} = \frac{1}{2\pi} \, \arctan\left(\frac{y_2}{y_1}\right).
\end{align}$$

This lets us obtain the requisite partial derivatives in a way that doesn't involve _too_ much algebra.
As an example, we have

$$\frac{\partial x_1}{\partial y_1} = -y_1 \exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)} = -y_1 x_1,$$

(where we used the product rule), and

$$\frac{\partial x_2}{\partial y_1} = \frac{1}{2\pi} \left(\frac{1}{1 + (y_2/y_1)^2}\right) \left(-\frac{y_2}{y_1^2}\right),$$

(where we used the chain rule, and the derivative $\mathrm{d}(\arctan(a))/\mathrm{d}a = 1/(1 + a^2)$).

Putting together the Jacobian matrix, we have:

$$\mathcal{J} = \begin{pmatrix}
-y_1 x_1 & -y_2 x_1 \\
-cy_2/y_1^2 & c/y_1 \\
\end{pmatrix},$$

where $c = [2\pi(1 + (y_2/y_1)^2)]^{-1}$.
The determinant of this matrix is

$$\begin{align}
\det(\mathcal{J}) &= -cx_1 - cx_1(y_2/y_1)^2 \\
&= -cx_1\left[1 + \left(\frac{y_2}{y_1}\right)^2\right] \\
&= -\frac{1}{2\pi} x_1 \\
&= -\frac{1}{2\pi}\exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)},
\end{align}$$

Coming right back to our probability density, we have that

$$\begin{align}
q(y_1, y_2) &= p(x_1, x_2) \cdot |\det(\mathcal{J})| \\
&= \frac{1}{2\pi}\exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)},
\end{align}$$

as desired.

::: {.callout-note}
We haven't yet explicitly accounted for the fact that $p(x_1, x_2)$ is 0 if either $x_1$ or $x_2$ are outside the range $(0, 1]$.
For example, if this constraint on $x_1$ and $x_2$ were to result in inaccessible values of $y_1$ or $y_2$, then $q(y_1, y_2)$ should be 0 for those values.
Formally, for the transformation $f: X \to Y$ where $X$ is the unit square (i.e. $0 < x_1, x_2 \leq 1$), $q(y_1, y_2)$ should only take the above value for the [image](https://en.wikipedia.org/wiki/Image_(mathematics)) of $f$, and anywhere outside of the image it should be 0.

In our case, the $\log(x_1)$ term in the transform varies between 0 and $\infty$, and the $\cos(2\pi x_2)$ term ranges from $-1$ to $1$.
Hence $y_1$, which is the product of these two terms, ranges from $-\infty$ to $\infty$, and likewise for $y_2$.
So the image of $f$ is the entire real plane, and we don't have to worry about this.
:::


## Bijectors.jl

All the above has purely been a mathematical discussion of how distributions can be transformed.
Now, we turn to their implementation in Julia, specifically using the [Bijectors.jl package](https://github.com/TuringLang/Bijectors.jl).

A _bijection_ between two sets ([Wikipedia](https://en.wikipedia.org/wiki/Bijection)) is, essentially, a one-to-one mapping between the elements of these sets.
That is to say, if we have two sets $X$ and $Y$, then a bijection maps each element of $X$ to a unique element of $Y$.
To return to our univariate example, where we transformed $x$ to $y$ using $y = \exp(x)$, the exponentiation function is a bijection because every value of $x$ maps to one unique value of $y$.
The input set (the domain) is $(-\infty, \infty)$, and the output set (the codomain) is $(0, \infty)$.

Since bijections are a one-to-one mapping between elements, we can also reverse the direction of this mapping to create an inverse function. 
In the case of $y = \exp(x)$, the inverse function is $x = \log(y)$.

::: {.callout-note}
Technically, the bijections in Bijectors.jl are functions $f: X \to Y$ for which:

 - $f$ is continuously differentiable, i.e. the derivative $\mathrm{d}f(x)/\mathrm{d}x$ exists and is continuous (over the domain of interest $X$);
- If $f^{-1}: Y \to X$ is the inverse of $f$, then that is also continuously differentiable (over _its_ own domain, i.e. $Y$).

The technical mathematical term for this is a diffeomorphism ([Wikipedia](https://en.wikipedia.org/wiki/Diffeomorphism)), but we call them 'bijectors'.

When thinking about continuous differentiability, it's important to be conscious of the domains or codomains that we care about.
For example, taking the inverse function $\log(y)$ from above, its derivative is $1/y$, which is not continuous at $y = 0$.
However, we specified that the bijection $y = \exp(x)$ maps values of $x \in (-\infty, \infty)$ to $y \in (0, \infty)$, so the point $y = 0$ is not within the domain of the inverse function.
:::

Specifically, one of the primary purposes of Bijectors.jl is used to construct _bijections which map constrained distributions to unconstrained ones_.
For example, the log-normal distribution which we saw above is constrained: its _support_, i.e. the range over which $p(x) \geq 0$, is $(0, \infty)$.
However, we can transform that to an unconstrained distribution (the normal distribution) using the transformation $y = \log(x)$.

The `bijector` function, when applied to a distribution, returns a bijection $f$ that can be used to map the constrained distribution to an unconstrained one.

```{julia}
import Bijectors as B

f = B.bijector(LogNormal())
```

We can apply this transformation to samples from the original distribution, for example:

```{julia}
samples_lognormal = rand(LogNormal(), 5)

samples_normal = f(samples_lognormal)
```

We can also obtain the inverse of a bijection, $f^{-1}$:

```{julia}
f_inv = B.inverse(f)

f_inv(samples_normal) == samples_lognormal
```

We know that the transformation $y = \log(x)$ changes the log-normal distribution to the normal distribution.
Bijectors.jl also gives us a way to access that transformed distribution:

```{julia}
transformed_dist = B.transformed(LogNormal(), f)
```

This type doesn't immediately look like a `Normal()`, but it behaves in exactly the same way.
For example, we can sample from it and plot a histogram:

```{julia}
samples_plot = rand(transformed_dist, 5000)
histogram(samples_plot, bins=50)
```

We can also obtain the logpdf of the transformed distribution and check that it is the same as that of a normal distribution:

```{julia}
println("Sample:   $(samples_plot[1])")
println("Expected: $(logpdf(Normal(), samples_plot[1]))")
println("Actual:   $(logpdf(transformed_dist, samples_plot[1]))")
```

Given the discussion in the previous sections, you might not be surprised to find that the transformed distribution is implemented using the Jacobian of the transformation.
Recall that

$$q(\mathbf{y}) = p(\mathbf{x}) \left| \det(\mathcal{J}) \right|,$$

where (if we assume that both $\mathbf{x}$ and $\mathbf{y}$ have length 2)

$$\mathcal{J} = \begin{pmatrix}
\partial x_1/\partial y_1 & \partial x_1/\partial y_2 \\
\partial x_2/\partial y_1 & \partial x_2/\partial y_2
\end{pmatrix}.$$

Slightly annoyingly, the convention in Bijectors.jl is the opposite way round compared to that in Bishop's book.
(Or perhaps it's annoying that Bishop's book uses the opposite convention!)
In Bijectors.jl, the Jacobian is defined as

$$\mathbf{J} = \begin{pmatrix}
\partial y_1/\partial x_1 & \partial y_1/\partial x_2 \\
\partial y_2/\partial x_1 & \partial y_2/\partial x_2
\end{pmatrix},$$

(note the partial derivatives have been flipped upside-down) and we have that

$$q(\mathbf{y})\left| \det(\mathbf{J}) \right| = p(\mathbf{x}),$$

or equivalently

$$\log(q(\mathbf{y})) = \log(p(\mathbf{x})) - \log(|\det(\mathbf{J})|).$$

You can access $\log(|\det(\mathbf{J})|)$ (evaluated at the point $\mathbf{x}$) using the `logabsdetjac` function:

```{julia}
# Reiterating the setup, just to be clear
x = rand(LogNormal())
f = B.bijector(LogNormal())
y = f(x)
transformed_dist = B.transformed(LogNormal(), f)

println("log(q(y))     : $(logpdf(transformed_dist, y))")
println("log(p(x))     : $(logpdf(LogNormal(), x))")
println("log(|det(J)|) : $(B.logabsdetjac(f, x))")
```

from which you can see that the equation above holds.
There are more functions available in the Bijectors.jl API; for full details do check out the [documentation](https://turinglang.org/Bijectors.jl/stable/).
For example, `logpdf_with_trans` can directly give us $\log(q(\mathbf{y}))$:

```{julia}
B.logpdf_with_trans(LogNormal(), x, true)
```

## The need for bijectors in MCMC

Constraints pose a problem for pretty much any kind of numerical method, and sampling is no exception to this.
The problem is that for any value $x$ outside of the support of a constrained distribution, $p(x)$ will be zero, and the logpdf will be $-\infty$.
Thus, any term that involves some ratio of probabilities (or equivalently, the logpdf)  will be infinite.

::: {.callout-note}
This post is already really long, and does not have quite enough space to explain either the Metropolis–Hastings or Hamiltonian Monte Carlo algorithms in detail.
If you need more information on these, please read e.g. chapter 11 of Bishop.
:::

### Metropolis–Hastings: fine?

This alone is not enough to cause issues for Metropolis–Hastings.
Here's an extremely barebones implementation of a random walk Metropolis algorithm:

```{julia}
# Take a step where the proposal is a normal distribution centred around
# the current value
function mh_step(p, x)
    x_proposed = rand(Normal(x, 1))
    acceptance_prob = min(1, p(x_proposed) / p(x))
    return if rand() < acceptance_prob
        x_proposed
    else
        x
    end
end

# Run a random walk Metropolis sampler.
# `p`  : a function that takes `x` and returns the pdf of the distribution
#        we're trying to sample from
# `x0` : the initial state
function mh(p, x0, n_samples)
    samples = []
    x = x0
    for _ in 2:n_samples
        x = mh_step(p, x)
        push!(samples, x)
    end
    return samples
end
```

With this we can sample from a log-normal distribution just fine:

```{julia}
p(x) = pdf(LogNormal(), x)
samples_with_mh = mh(p, 1.0, 5000)
histogram(samples_with_mh, bins=0:0.1:5; xlims=(0, 5))
```

In this MH implementation, the only place where $p(x)$ comes into play is in the acceptance probability.

As long as we make sure to start the sampling at a point within the support of the distribution, `p(x)` will be nonzero.
If the proposal step generates an `x_proposal` that is outside the support, `p(x_proposal)` will be zero, and the acceptance probability (`p(x_proposal)/p(x)`) will be zero.
So such a step will never be accepted, and the sampler will continue to stay within the support of the distribution.

Although this does mean that we may find ourselves having a higher reject rate than usual, and thus less efficient sampling, it at least does not cause the algorithm to become unstable or crash.

### Hamiltonian Monte Carlo: not so fine

The _real_ problem comes with gradient-based methods like Hamiltonian Monte Carlo (HMC).
Here's an equally barebones implementation of HMC.

```{julia}
using LinearAlgebra: I
import ForwardDiff

# Really basic leapfrog integrator.
# `z`        : position
# `r`        : momentum
# `timestep` : size of one integration step
# `nsteps`   : number of integration steps
# `dEdz`     : function that returns the derivative of the energy with respect
#              to `z`. The energy is the negative logpdf of the distribution
#              we're trying to sample from.
function leapfrog(z, r, timestep, nsteps, dEdz)
    function step_inner(z, r)
        # One small step for r, one giant leap for z
        r -= (timestep / 2) * dEdz(z)
        z += timestep * r
        # (and then one more small step for r)
        r -= (timestep / 2) * dEdz(z)
        return (z, r)
    end
    for _ in 1:nsteps
        z, r = step_inner(z, r)
    end
    (isnan(z) || isnan(r)) && error("Numerical instability encountered in leapfrog")
    return (z, -r)
end

# Take one HMC step.
# `z` : current position
# `E` : function that returns the energy (negative logpdf) at `z`
# Other arguments are as above
function hmc_step(z, E, dEdz, integ_timestep, integ_nsteps)
    # Generate new momentum
    r = randn()
    # Integrate the Hamiltonian dynamics
    z_new, r_new = leapfrog(z, r, integ_timestep, integ_nsteps, dEdz)
    # Calculate Hamiltonian
    H = E(z) + 0.5 * sum(r .^ 2)
    H_new = E(z_new) + 0.5 * sum(r_new .^ 2)
    # Acceptance criterion
    accept_prob = min(1, exp(H - H_new))
    return if rand() < accept_prob
        z_new
    else
        z
    end
end

# Run HMC.
# `z0` : initial position
# Other arguments are as above
function hmc(z0, E, dEdz, nsteps; integ_timestep=0.1, integ_nsteps=100)
    samples = [z0]
    z = z0
    for _ in 2:nsteps
        z = hmc_step(z, E, dEdz, integ_timestep, integ_nsteps)
        push!(samples, z)
    end
    return samples
end
```

Okay, that's our HMC set up.
Now, let's try to sample from a log-normal distribution:

```{julia}
#| error: true
p(x) = pdf(LogNormal(), x)
E(x) = -log(p(x))
dEdz(x) = ForwardDiff.derivative(E, x)
samples_with_hmc = hmc(1.0, E, dEdz, 5000)
histogram(samples_with_hmc, bins=0:0.1:5; xlims=(0, 5))
```

Eeeek! What happened?
It turns out that evaluating the gradient of the energy at any point outside the support of the distribution is not possible:

```{julia}
dEdz(-1)
```

This is because $p(x)$ is 0, and hence $E(x) = -\log(p(x))$ is $\infty$ outside the support.
If we try to evaluate the gradient at such a point, it's simply undefined, because arithmetic on infinity doesn't make sense:

```{julia}
Inf - Inf
```

To really pinpoint where this is happening, we need to look into the HMC leapfrog integration, specifically these lines:

```julia
r -= (timestep / 2) * dEdz(z)   #  (1)
z += timestep * r               #  (2)
r -= (timestep / 2) * dEdz(z)   #  (3)
```

Here, `z` is the position and `r` the momentum.
Since we start our sampler inside the support of the distribution (by supplying a good initial point), `dEdz(z)` will start off being well-defined on line (1).
However, after `r` is updated on line (1), `z` is updated again on line (2), and _this_ value of `z` may well be outside of the support.
At this point, `dEdz(z)` will be `NaN`, and the final update to `r` on line (3) will also cause it to be `NaN`.

Even if we're lucky enough for an individual integration step to not move `z` outside the support, there are many integration steps per sampler step, and many sampler steps, and so the chances of this happening at some point are quite high.

It's possible to choose our integration parameters carefully to reduce the risk of this happening.
For example, we could set the integration timestep to be _really_ small, thus reducing the chance of making a move outside the support.
But that will just lead to a very slow exploration of parameter space, and in general, we should like to avoid this problem altogether.

### Rescuing HMC

Perhaps unsurprisingly, the answer to this is to transform the underlying distribution to an unconstrained one and sample from that instead.
However, we have to make sure that we include the pesky Jacobian term when sampling from the transformed distribution.
That's where Bijectors.jl can come in.

The main change we need to make is to pass a modified version of the function `p` to our HMC sampler.
Recall back at the very start, we transformed $p(x)$ into $q(y)$, and said that

$$q(y) = p(x) \left| \frac{\mathrm{d}x}{\mathrm{d}y} \right|.$$

What we want the HMC sampler to see is the transformed distribution $q(y)$, not the original distribution $p(x)$.
And Bijectors.jl lets us calculate $\log(q(y))$ using `logpdf_with_trans(p, x, true)`:

```{julia}
d = LogNormal()
f = B.bijector(d)     # Transformation function
f_inv = B.inverse(f)  # Inverse transformation function

function logq(y)
    x = f_inv(y)
    return B.logpdf_with_trans(d, x, true)
end
# These definitions are the same as before, except that
# the call to `log` has been moved up into logq rather
# than in E.
E(z) = -logq(z)
dEdz(z) = ForwardDiff.derivative(E, z)
```

Now, because our transformed distribution is unconstrained, we can evaluate `E` and `dEdz` at any point, and sample with more confidence:

```{julia}
samples_with_hmc = hmc(1.0, E, dEdz, 5000)
samples_with_hmc[1:5]
```

No sampling errors this time... yay!
We have to remember that when we run HMC on this, it will give us back samples of `y`, not `x`.
So we can untransform them:

```{julia}
samples_with_hmc_untransformed = f_inv(samples_with_hmc)
histogram(samples_with_hmc_untransformed, bins=0:0.1:5; xlims=(0, 5))
```

We can also check that the mean and variance of the samples are what we expect them to be.
From [Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution), the mean and variance of a log-normal distribution are respectively $\exp(\mu + \sigma^2/2)$ and $[\exp(\sigma^2) - 1]\exp(2\mu + \sigma^2)$.
For our log-normal distribution, we set $\mu = 0$ and $\sigma = 1$, so the mean and variance should be $1.6487$ and $4.6707$ respectively.

```{julia}
println("    mean : $(mean(samples_with_hmc_untransformed))")
println("variance : $(var(samples_with_hmc_untransformed))")
```

::: {.callout-note}
You might notice that the variance is a little bit off.
The truth is that it's actually quite tricky to get an accurate variance when sampling from a log-normal distribution.
You can see this even with Turing.jl itself:

```{julia}
using Turing
setprogress!(false)
@model ln() = x ~ LogNormal()
chain = sample(ln(), HMC(0.2, 3), 5000)
(mean(chain[:x]), var(chain[:x]))
```
:::

The importance of the Jacobian term here isn't to enable sampling _per se_.
Because the resulting distribution is unconstrained, we could have still sampled from it without using the Jacobian.
However, adding the Jacobian is what ensures that when we un-transform the samples, we get the correct distribution.

This is what happens if we don't include the Jacobian term.
In this `logq_wrong`, we've un-transformed `y` to `x` and calculated the logpdf with respect to its original distribution.
This is exactly the same mistake that we made at the start of this article with `naive_logpdf`.

```{julia}
function logq_wrong(y)
    x = f_inv(y)
    return logpdf(d, x)
end
E(z) = -logq_wrong(z)
dEdz(z) = ForwardDiff.derivative(E, z)
samples_questionable = hmc(1.0, E, dEdz, 5000)
samples_questionable_untransformed = f_inv(samples_questionable)

println("    mean : $(mean(samples_questionable_untransformed))")
println("variance : $(var(samples_questionable_untransformed))")
```

You can see that even though the sampling ran fine without errors, the summary statistics are completely wrong.

## How does DynamicPPL use bijectors?

In the final section of this article, we'll discuss the higher-level implications of constrained distributions in the Turing.jl framework.

When we are performing Bayesian inference, we're trying to sample from a joint probability distribution, which isn't usually a single, well-defined distribution like in the rather simplified example above.
However, each random variable in the model will have its own distribution, and often some of these will be constrained.
For example, if `b ~ LogNormal()` is a random variable in a model, then $p(b)$ will be zero for any $b \leq 0$.
Consequently, any joint probability $p(b, c, \ldots)$ will also be zero for any combination of parameters where $b \leq 0$, and so that joint distribution is itself constrained.

To get around this, DynamicPPL allows the variables to be transformed in exactly the same way as above.
For simplicity, consider the following model:

```{julia}
using DynamicPPL

@model function demo()
    x ~ LogNormal()
end

model = demo()
vi = VarInfo(model)
vn_x = @varname(x)
# Retrieve the 'internal' value of x – we'll explain this later
DynamicPPL.getindex_internal(vi, vn_x)
```

The call to `VarInfo` executes the model once and stores the sampled value inside `vi`.
By default, `VarInfo` itself stores un-transformed values.
We can see this by comparing the value of the logpdf stored inside the `VarInfo`:

```{julia}
DynamicPPL.getlogp(vi)
```

with a manual calculation:

```{julia}
logpdf(LogNormal(), DynamicPPL.getindex_internal(vi, vn_x))
```

In DynamicPPL, the `link` function can be used to transform the variables.
This function does three things: firstly, it transforms the variables; secondly, it updates the value of logp (by adding the Jacobian term); and thirdly, it sets a flag on the variables to indicate that it has been transformed.
Note that this acts on _all_ variables in the model, including unconstrained ones.
(Unconstrained variables just have an identity transformation.)

```{julia}
vi_linked = DynamicPPL.link(vi, model)
println("Transformed value: $(DynamicPPL.getindex_internal(vi_linked, vn_x))")
println("Transformed logp: $(DynamicPPL.getlogp(vi_linked))")
println("Transformed flag: $(DynamicPPL.istrans(vi_linked, vn_x))")
```

Indeed, we can see that the new logp value matches with

```{julia}
logpdf(Normal(), DynamicPPL.getindex_internal(vi_linked, vn_x))
```

The reverse transformation, `invlink`, reverts all of the above steps:

```{julia}
vi = DynamicPPL.invlink(vi_linked, model)  # Same as the previous vi
println("Un-transformed value: $(DynamicPPL.getindex_internal(vi, vn_x))")
println("Un-transformed logp: $(DynamicPPL.getlogp(vi))")
println("Un-transformed flag: $(DynamicPPL.istrans(vi, vn_x))")
```

### Values and 'internal' values

In DynamicPPL, there is a difference between the value of a random variable and its 'internal' value.
This is most easily seen by first transforming, and then comparing the output of `getindex` and `getindex_internal`.
The former extracts the regular value, whereas (as the name suggests) the latter gets the 'internal' value.

```{julia}
println("Value: $(getindex(vi_linked, vn_x))")  # same as `vi_linked[vn_x]`
println("Internal value: $(DynamicPPL.getindex_internal(vi_linked, vn_x))")
```

We can see (for the linked varinfo) that there are _two_ differences between these outputs:

1. _The internal value has been transformed using the bijector (in this case, the log function)._
   This means that the `istrans()` flag which we used above doesn't tell us anything about whether the 'external' value has been transformed: it only tells us about the internal value.

2. _The internal value is a vector, whereas the value is a scalar._
   This is because _all_ internal values are vectorised (i.e. converted into some vector), regardless of distribution.

   | Distribution                     | Value  | Internal value                          |
   | ---                              | ---    | ---                                     |
   | Univariate (e.g. `Normal()`)     | Scalar | Length-1 vector, possibly transformed   |
   | Multivariate (e.g. `MvNormal()`) | Vector | Vector, possibly transformed            |
   | Matrixvariate (e.g. `Wishart()`) | Matrix | Vectorised matrix, possibly transformed |

Essentially, the value is the one which the user 'expects' to see based on the model definition.
The 'internal' value is one that is the most convenient representation to work with inside DynamicPPL.

It also means that internally, the transformation in `link` is carried out in three steps:

1. Un-vectorise the internal value.
2. Apply the transformation.
3. Vectorise the transformed value.

The actual implementation is slightly harder to parse as it has to work for different flavours of `VarInfo`, but it eventually boils down to the following (see the implementation [here](https://github.com/TuringLang/DynamicPPL.jl/blob/ba490bf362653e1aaefe298364fe3379b60660d3/src/varinfo.jl#L1390-L1414)):

```{julia}
# Use the un-linked varinfo
dist = DynamicPPL.getdist(vi, vn_x)
x_val = DynamicPPL.getindex_internal(vi, vn_x)
```

```{julia}
# Step 1: un-vectorise
fn1 = DynamicPPL.from_vec_transform(dist)
fn1(x_val)
```

```{julia}
# Step 2: transform
# DynamicPPL.link_transform(dist) is really Bijectors.bijector(dist)
fn2 = DynamicPPL.link_transform(dist)
fn2(fn1(x_val))
```

```{julia}
# Step 3.: re-vectorise
fn3 = DynamicPPL.to_vec_transform(dist)
fn3(fn2(fn1(x_val)))
```

## Sampling in Turing.jl

DynamicPPL provides the _functionality_ for transforming variables, but the transformation itself happens at an even higher level, i.e. in the sampler itself.
For example, consider the HMC sampler in Turing.jl, which is in [this file](https://github.com/TuringLang/Turing.jl/blob/5b24cebe773922e0f3d5c4cb7f53162eb758b04d/src/mcmc/hmc.jl).
In the first step of sampling, it calls `link` on the sampler.
This transformation is preserved throughout the sampling process, meaning that `istrans()` always returns true.

We can observe this by inserting print statements into the model.
Here, `__varinfo__` is the internal symbol for the `VarInfo` object used in model evaluation:

```{julia}
@model function demo2()
    x ~ LogNormal()
    if x isa Float64
        println("-----------")
        println("value: $x")
        println("internal value: $(DynamicPPL.getindex_internal(__varinfo__, @varname(x)))")
        println("istrans: $(istrans(__varinfo__, @varname(x)))")
    end
end

sample(demo2(), HMC(0.1, 3), 3);
```

(Here, the check on `if x isa Float64` prevents the printing from occurring during computation of the derivative.)
You can see that during the actual sampling, `istrans` is always kept as `true`.

::: {.callout-note}
The first two model evaluations where `istrans` is `false` occur prior to the actual sampling.
One occurs when the model is checked for correctness (using [`DynamicPPL.check_model_and_trace`](https://github.com/TuringLang/DynamicPPL.jl/blob/ba490bf362653e1aaefe298364fe3379b60660d3/src/debug_utils.jl#L582-L612)).
The second occurs because the model is evaluated once to generate a set of initial parameters inside [DynamicPPL's implementation of `AbstractMCMC.step`](https://github.com/TuringLang/DynamicPPL.jl/blob/ba490bf362653e1aaefe298364fe3379b60660d3/src/sampler.jl#L98-L117).
Both of these steps occur with all samplers in Turing.jl.
:::

What this means is that from the perspective of the HMC sampler, it _never_ sees the constrained variable: it always thinks that it is sampling from an unconstrained distribution.

The biggest prerequisite for this to work correctly is that the potential energy term in the Hamiltonian—or in other words, the model log density—must be programmed correctly to include the Jacobian term.
This is exactly the same as how we had to make sure to define `logq(y)` correctly in the toy HMC example above.

This occurs correctly because a statement like `x ~ LogNormal()` in the model definition above is translated into `assume(LogNormal(), @varname(x), __varinfo__)`, defined [here](https://github.com/TuringLang/DynamicPPL.jl/blob/ba490bf362653e1aaefe298364fe3379b60660d3/src/context_implementations.jl#L225-L229).
As can be seen by following through on the definition of `invlink_with_logpdf`, this does indeed checks for the presence of the `istrans` flag and adds the Jacobian accordingly.

::: {.callout-note}
The discussion above skips over several steps in the Turing.jl codebase, which can be difficult to follow.
Specifically:

1. Samplers such as HMC [wrap Turing models in a `DynamicPPL.LogDensityFunction`](https://github.com/TuringLang/Turing.jl/blob/5b24cebe773922e0f3d5c4cb7f53162eb758b04d/src/mcmc/hmc.jl#L159-L168).
2. The log density at a given set of parameter values can then be [calculated using `logdensity`](https://github.com/TuringLang/DynamicPPL.jl/blob/ba490bf362653e1aaefe298364fe3379b60660d3/src/logdensityfunction.jl#L136-L141)
3. This in turn calls `evaluate!!`, which runs the _model evaluator function_. This evaluator function is not visible in the DynamicPPL codebase because it is generated by the expansion of the `@model` macro. You can see it, though, by running:
   ```julia
   @macroexpand @model demo3() = x ~ LogNormal()
   ```
   Note that these evaluations do not trigger the print statements in the model because it is run using automatic differentiation (in this case, `x` is a `ForwardDiff.Dual`).
4. This generates a line which looks like 
   ```julia
   (var"##value#441", __varinfo__) = (DynamicPPL.tilde_assume!!)(__context__, (DynamicPPL.unwrap_right_vn)((DynamicPPL.check_tilde_rhs)(var"##dist#440"), var"##vn#437")..., __varinfo__)
   ```
   `tilde_assume!!` in turn calls `tilde_assume`, which ultimately delegates to `assume`.
:::
