---
title: "Transforms and distributions"
engine: julia
---

```{julia}
import Random
Random.seed!(468);
```

This article seeks to motivate Bijectors.jl and how distributions are transformed in the Turing.jl probabilistic programming language.

It assumes:

- some basic knowledge of probability distributions (the notions of sampling from them and calculating the probability density function for a given distribution); and
- some calculus (the chain and product rules for differentiation, and changes of variables in integrals).

## Sampling from a distribution

To sample from a distribution (as defined in Distributions.jl), we can use the `rand` function.
Let's sample from a normal distribution and then plot a histogram of the samples.
Calling `Normal()` with no arguments gives a standard normal distribution with mean 0 and standard deviation 1.

```{julia}
using Distributions
using Plots

samples = rand(Normal(), 5000)
histogram(samples, bins=50)
```

That's all great, and furthermore if you want to know the probability of observing any of the samples, you can use `logpdf`:

```{julia}
(samples[1], logpdf(Normal(), samples[1]))
```

The probability density function for the normal distribution with mean 0 and standard deviation 1 is

$$p(x) = \frac{1}{\sqrt{2\pi}} \exp{\left(-\frac{x^2}{2}\right)},$$

so we could also have calculated this manually using:

```{julia}
log(1 / sqrt(2π) * exp(-samples[1]^2 / 2))
```

## Sampling from a transformed distribution

Now say that $x$ is distributed according to `Normal()`, and we want to draw samples from $y = \exp(x)$.
The distribution of $y$ is known as a [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution).

For illustration purposes, let's make our own `MyLogNormal` distribution that we can sample from: see Distribution.jl's documentation on custom distributions [here](https://juliastats.org/Distributions.jl/stable/extends/#Univariate-Distribution).
(Distributions already defines its own `LogNormal`, so we have to use a different name.)

```{julia}
struct MyLogNormal <: ContinuousUnivariateDistribution
    μ::Float64
    σ::Float64
end
MyLogNormal() = MyLogNormal(0.0, 1.0)

Base.rand(rng::Random.AbstractRNG, d::MyLogNormal) = exp(rand(rng, Normal(d.μ, d.σ)))
```

Great, now we can do the same as above:

```{julia}
samples_lognormal = rand(MyLogNormal(), 5000)
# Cut off the tail for clearer visualization
histogram(samples_lognormal, bins=0:0.1:5; xlims=(0, 5))
```

How do we implement `logpdf` for our new distribution, though?

Naively, we might think to just un-transform the variable `y`, and then use the `logpdf` of the normal distribution.

```{julia}
bad_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))
```

We can compare this function against the logpdf implemented in Distributions.jl.
(The name chosen here certainly foreshadows that it's not going to be correct, though!)

```{julia}
println("Sample   : $(samples_lognormal[1])")
println("Expected : $(logpdf(LogNormal(), samples_lognormal[1]))")
println("Actual   : $(bad_logpdf(MyLogNormal(), samples_lognormal[1]))")
```

## The derivative

Fundamentally, the reason why this doesn't work is because transforming a (continuous) distribution causes probability density to be stretched and otherwise moved around.

::: {.callout-note}
There are various posts on the Internet that explain this visually; I'm too lazy to draw a diagram _right now_, but I might do it later.
:::

I personally find it most useful to not talk about probability density itself, but instead to make it more concrete by talking about actual probabilities.
If we think about the normal distribution as a continuous curve, what the probability density function $p(x)$ really tells us is that for any two points $a$ and $b$ (where $a \leq b$), the probability of drawing a sample from the interval $[a, b]$ is the area under the curve, i.e.

$$\int_a^b p(x) \, \mathrm{d}x.$$

For example, if $(a, b) = (-\infty, \infty)$, then the probability of drawing a sample from the entire distribution is 1.

Let's say that the probability density function of the log-normal distribution is $q(y)$.
Then, the area under the curve between the two points $\exp(a)$ and $\exp(b)$ is:

$$\int_{\exp(a)}^{\exp(b)} q(y) \, \mathrm{d}y.$$

This integral should be equal to the one above, because the probability of drawing from $[a, b]$ in the original distribution should be the same as the probability of drawing from $[\exp(a), \exp(b)]$ in the transformed distribution.
The question we have to solve here is: how do we find a function $q(y)$ such that this equality holds?

We can approach this by substituting $y = \exp(x)$ into the first integral (see [Wikipedia](https://en.wikipedia.org/wiki/Integration_by_substitution) for a refresher if needed).
We have that:

$$\frac{\mathrm{d}y}{\mathrm{d}x} = \exp(x) = y \implies \mathrm{d}x = \frac{1}{y}\,\mathrm{d}y$$

and so

$$\int_{x=a}^{x=b} p(x) \, \mathrm{d}x
  \longrightarrow \int_{y=\exp(a)}^{y=\exp(b)} \underbrace{p(\log(y)) \frac{1}{y}}_{q(y)} \,\mathrm{d}y,$$

from which we can read off $q(y) = p(\log(y)) / y$.

In contrast, when we implemented `bad_logpdf`

```{julia}
bad_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))
```

that was the equivalent of saying that $q(y) = p(\log(y))$.
We left out a factor of $1/y$!

Indeed, now we can define the correct `logpdf` function.
Since everything is a logarithm here, instead of multiplying by $1/y$ we subtract $\log(y)$:

```{julia}
Distributions.logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y)) - log(y)
```

and check that it works:

```{julia}
println("Sample   : $(samples_lognormal[1])")
println("Expected : $(logpdf(LogNormal(), samples_lognormal[1]))")
println("Actual   : $(logpdf(MyLogNormal(), samples_lognormal[1]))")
```

The same logic can be applied to _any_ kind of transformation.
If we have some transformation from $x$ to $y$, and the probability density functions of $x$ and $y$ are $p(x)$ and $q(y)$ respectively, then

$$q(y) = p(x) \left| \frac{\mathrm{d}x}{\mathrm{d}y} \right|.$$

In this case, we had $y = \exp(x)$, so $\mathrm{d}x/\mathrm{d}y = 1/y$.
This equation is (11.5) in Bishop's textbook.

::: {.callout-note}
The absolute value here takes care of the case where $f$ is decreasing, i.e., the distribution is flipped.
You can try this out with the transformation $y = -\exp(x)$.
If $a < b$, then $-exp(a) > -exp(b)$, and so you will have to swap the integration limits to ensure that the integral comes out positive.
:::

## The Jacobian

In general, we may have transforms that act on multivariate distributions, for example something mapping $p(x_1, x_2)$ to $q(y_1, y_2)$.
In this case, the rule above has to be extended by replacing the derivative $\mathrm{d}x/\mathrm{d}y$ with the determinant of the Jacobian matrix:

$$\mathcal{J} = \begin{pmatrix}
\partial x_1/\partial y_1 & \partial x_1/\partial y_2 \\
\partial x_2/\partial y_1 & \partial x_2/\partial y_2
\end{pmatrix}.$$

and specifically,

$$q(y_1, y_2) = p(x_1, x_2) \left| \det(\mathcal{J}) \right|.$$

This is the same as equation (11.9) in Bishop, except that he denotes the absolute value of the determinant with just $|\mathcal{J}|$.

::: {.callout-note}
In different contexts the Jacobian can have different 'numerators' and 'denominators' in the partial derivatives.
For example, if $\mathbf{y} = f(\mathbf{x})$, then it's common to write $\mathbf{J}$ as a matrix of partial derivatives of elements of $y$ with respect to elements of $x$.
Indeed, later in this article we will see that Bijectors.jl uses this convention.

It is always the case, though, that the elements of the 'numerator' vary with rows and the elements of the 'denominator' vary with columns.
:::

The rest of this section will be devoted to an example to show that this works, and contains some slightly less pretty mathematics.
If you are already suitably convinced by this stage, then you can skip the rest of this section.
(Or if you prefer something more formal, the Wikipedia article on integration by substitution [discusses the multivariate case as well](https://en.wikipedia.org/wiki/Integration_by_substitution#Substitution_for_multiple_variables).)

### An example: the Box–Muller transform

A motivating example where one might like to use a Jacobian is the [Box–Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform), which is a technique for sampling from a normal distribution.

The Box–Muller transform works by first sampling two random variables from the uniform distribution between 0 and 1:

$$\begin{align}
x_1 &\sim U(0, 1) \\
x_2 &\sim U(0, 1).
\end{align}$$

Both of these have a probability density function of $p(x) = 1$ for $0 < x \leq 1$, and 0 otherwise.
Because they are independent, we can write that

$$p(x_1, x_2) = p(x_1) p(x_2) = \begin{cases}
1 & \text{if } 0 < x_1 \leq 1 \text{ and } 0 < x_2 \leq 1, \\
0 & \text{otherwise}.
\end{cases}$$

The next step is to perform the transforms

$$\begin{align}
y_1 &= \sqrt{-2 \log(x_1)} \cos(2\pi x_2); \\
y_2 &= \sqrt{-2 \log(x_1)} \sin(2\pi x_2),
\end{align}$$

and it turns out that with these transforms, both $y_1$ and $y_2$ are independent and normally distributed with mean 0 and standard deviation 1, i.e.

$$q(y_1, y_2) = \frac{1}{2\pi} \exp{\left(-\frac{y_1^2}{2}\right)} \exp{\left(-\frac{y_2^2}{2}\right)}.$$

How can we show that this is the case?

There are many ways to work out the required calculus.
Some are more elegant and some rather less so!
One of the less headache-inducing ways is to define the intermediate variables:

$$r = \sqrt{-2 \log(x_1)}; \quad \theta = 2\pi x_2,$$

from which we can see that $y_1 = r\cos\theta$ and $y_2 = r\sin\theta$, and hence

$$\begin{align}
x_1 &= \exp{\left(-\frac{r^2}{2}\right)} = \exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)}; \\
x_2 &= \frac{\theta}{2\pi} = \frac{1}{2\pi} \, \arctan\left(\frac{y_2}{y_1}\right).
\end{align}$$

This lets us obtain the requisite partial derivatives in a way that doesn't involve _too_ much algebra.
As an example, we have

$$\frac{\partial x_1}{\partial y_1} = -y_1 \exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)} = -y_1 x_1,$$

(where we used the product rule), and

$$\frac{\partial x_2}{\partial y_1} = \frac{1}{2\pi} \left(\frac{1}{1 + (y_2/y_1)^2}\right) \left(-\frac{y_2}{y_1^2}\right),$$

(where we used the chain rule, and the derivative $\mathrm{d}(\arctan(a))/\mathrm{d}a = 1/(1 + a^2)$).

Putting together the Jacobian matrix, we have:

$$\mathcal{J} = \begin{pmatrix}
-y_1 x_1 & -y_2 x_1 \\
-cy_2/y_1^2 & c/y_1 \\
\end{pmatrix},$$

where $c = [2\pi(1 + (y_2/y_1)^2)]^{-1}$.
The determinant of this matrix is

$$\begin{align}
\det(\mathcal{J}) &= -cx_1 - cx_1(y_2/y_1)^2 \\
&= -cx_1\left[1 + \left(\frac{y_2}{y_1}\right)^2\right] \\
&= -\frac{1}{2\pi} x_1 \\
&= -\frac{1}{2\pi}\exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)},
\end{align}$$

Coming right back to our probability density, we have that

$$\begin{align}
q(y_1, y_2) &= p(x_1, x_2) \cdot |\det(\mathcal{J})| \\
&= \frac{1}{2\pi}\exp{\left(-\frac{y_1^2}{2}\right)}\exp{\left(-\frac{y_2^2}{2}\right)},
\end{align}$$

as desired.

::: {.callout-note}
We haven't yet explicitly accounted for the fact that $p(x_1, x_2)$ is 0 if either $x_1$ or $x_2$ are outside the range $(0, 1]$.
For example, if this constraint on $x_1$ and $x_2$ were to result in inaccessible values of $y_1$ or $y_2$, then $q(y_1, y_2)$ should be 0 for those values.
Formally, for the transformation $f: X \to Y$ where $X$ is the unit square (i.e. $0 < x_1, x_2 \leq 1$), $q(y_1, y_2)$ should only take the above value for the [image](https://en.wikipedia.org/wiki/Image_(mathematics)) of $f$, and anywhere outside of the image it should be 0.

In our case, the $\log(x_1)$ term in the transform varies between 0 and $\infty$, and the $\cos(2\pi x_2)$ term ranges from $-1$ to $1$.
Hence $y_1$, which is the product of these two terms, ranges from $-\infty$ to $\infty$, and likewise for $y_2$.
So the image of $f$ is the entire real plane, and we don't have to worry about this.
:::


## Bijectors.jl

All the above has purely been a mathematical discussion of how distributions can be transformed.
Now, we turn to their implementation in Julia, specifically using the [Bijectors.jl package](https://github.com/TuringLang/Bijectors.jl).

A _bijection_ between two sets ([Wikipedia](https://en.wikipedia.org/wiki/Bijection)) is, essentially, a one-to-one mapping between the elements of these sets.
That is to say, if we have two sets $X$ and $Y$, then a bijection maps each element of $X$ to a unique element of $Y$.
To return to our univariate example, where we transformed $x$ to $y$ using $y = \exp(x)$, the exponentiation function is a bijection because every value of $x$ maps to one unique value of $y$.
The input set (the domain) is $(-\infty, \infty)$, and the output set (the codomain) is $(0, \infty)$.

Since bijections are a one-to-one mapping between elements, we can also reverse the direction of this mapping to create an inverse function. 
In the case of $y = \exp(x)$, the inverse function is $x = \log(y)$.

::: {.callout-note}
Technically, the bijections in Bijectors.jl are functions $f: X \to Y$ for which:

 - $f$ is continuously differentiable, i.e. the derivative $\mathrm{d}f(x)/\mathrm{d}x$ exists and is continuous (over the domain of interest $X$);
- If $f^{-1}: Y \to X$ is the inverse of $f$, then that is also continuously differentiable (over _its_ own domain, i.e. $Y$).

These are called diffeomorphisms ([Wikipedia](https://en.wikipedia.org/wiki/Diffeomorphism)).

When thinking about continuous differentiability, it's important to be conscious of the domains or codomains that we care about.
For example, taking the inverse function $\log(y)$ from above, its derivative is $1/y$, which is not continuous at $y = 0$.
However, we specified that the bijection $y = \exp(x)$ maps values of $x \in (-\infty, \infty)$ to $y \in (0, \infty)$, so the point $y = 0$ is not within the domain of the inverse function.
:::

It's not entirely clear to me who first coined the term biject**or** (as opposed to biject**ion**), which is the mathematical term.
As far as I can tell, it's only used in this specific context of transforming probability distributions, and apart from Bijectors.jl itself, it is also used in [the TensorFlow deep learning framework](https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors).

Specifically, one of the primary purposes of Bijectors.jl is used to construct _bijections which map constrained distributions to unconstrained ones_.
For example, the log-normal distribution which we saw above is constrained: its _support_, i.e. the range over which $p(x) \geq 0$, is (0, $\infty$).
However, we can transform that to an unconstrained distribution (the normal distribution) using the transformation $y = \log(x)$.
The `bijector` function, when applied to a distribution, returns a bijection $f$ that can be used to map the constrained distribution to an unconstrained one.

```{julia}
import Bijectors as B

f = B.bijector(LogNormal())
```

We can apply this transformation to samples from the original distribution, for example:

```{julia}
samples_lognormal = rand(LogNormal(), 5)

samples_normal = f(samples_lognormal)
```

We can also obtain the inverse of a bijection, $f^{-1}$:

```{julia}
f_inv = B.inverse(f)

f_inv(samples_normal) == samples_lognormal
```

We know that the transformation $y = \log(x)$ changes the log-normal distribution to the normal distribution.
Bijectors.jl also gives us a way to access that transformed distribution:

```{julia}
transformed_dist = B.transformed(LogNormal(), f)
```

This type doesn't immediately look like a `Normal()`, but it behaves in exactly the same way.
For example, we can sample from it and plot a histogram:

```{julia}
samples_plot = rand(transformed_dist, 5000)
histogram(samples_plot, bins=50)
```

We can also obtain the logpdf of the transformed distribution and check that it is the same as that of a normal distribution:

```{julia}
println("Sample:   $(samples_plot[1])")
println("Expected: $(logpdf(Normal(), samples_plot[1]))")
println("Actual:   $(logpdf(transformed_dist, samples_plot[1]))")
```

Given the discussion in the previous sections, you might not be surprised to find that the transformed distribution is implemented using the Jacobian of the transformation.
Recall that

$$q(\mathbf{y}) = p(\mathbf{x}) \left| \det(\mathcal{J}) \right|,$$

where (if we assume that both $\mathbf{x}$ and $\mathbf{y}$ have length 2)

$$\mathcal{J} = \begin{pmatrix}
\partial x_1/\partial y_1 & \partial x_1/\partial y_2 \\
\partial x_2/\partial y_1 & \partial x_2/\partial y_2
\end{pmatrix}.$$

Slightly annoyingly, the convention in Bijectors.jl is the opposite way round compared to that in Bishop's book.
(Or perhaps it's annoying that Bishop's book uses the opposite convention!)
In Bijectors.jl, the Jacobian is defined as

$$\mathbf{J} = \begin{pmatrix}
\partial y_1/\partial x_1 & \partial y_1/\partial x_2 \\
\partial y_2/\partial x_1 & \partial y_2/\partial x_2
\end{pmatrix},$$

(note the partial derivatives have been flipped upside-down) and we have that

$$q(\mathbf{y})\left| \det(\mathbf{J}) \right| = p(\mathbf{x}),$$

or equivalently

$$\log(q(\mathbf{y})) = \log(p(\mathbf{x})) - \log(|\det(\mathbf{J})|).$$

You can access $\log(|\det(\mathbf{J})|)$ (evaluated at the point $\mathbf{x}$) using the `logabsdetjac` function:

```{julia}
# Reiterating the setup, just to be clear
x = rand(LogNormal())
f = B.bijector(LogNormal())
y = f(x)
transformed_dist = B.transformed(LogNormal(), f)

println("log(q(y))     : $(logpdf(transformed_dist, y))")
println("log(p(x))     : $(logpdf(LogNormal(), x))")
println("log(|det(J)|) : $(B.logabsdetjac(f, x))")
```

from which you can see that the equation above holds.
There are more functions available in the Bijectors.jl API; for full details do check out the [documentation](https://turinglang.org/Bijectors.jl/stable/).
For example, `logpdf_with_trans` can directly give us $\log(q(\mathbf{y}))$:

```{julia}
B.logpdf_with_trans(LogNormal(), x, true)
```

## Why is this useful for sampling anyway?

Constrained vs unconstrained variables, sampling, etc.

## How does DynamicPPL use bijectors?

link, invlink, transform, varinfo etc.

See [https://turinglang.org/DynamicPPL.jl/stable/internals/transformations/](https://turinglang.org/DynamicPPL.jl/stable/internals/transformations/)
