---
title: "Transforms and distributions"
engine: julia
---

```{julia}
import Random
Random.seed!(468);

```

This article is about transforming distributions and Bijectors.jl.

## Sampling from a distribution

To sample from a distribution (as defined in Distributions.jl), we can use the `rand` function.
Let's sample from a normal distribution and then plot a histogram of the samples.
Calling `Normal()` with no arguments gives a standard normal distribution with mean 0 and standard deviation 1.

```{julia}
using Distributions
using Plots

samples = rand(Normal(), 5000)
histogram(samples, bins=50)
```

That's all great, and furthermore if you want to know the probability of observing any of the samples, you can use `logpdf`:

```{julia}
(samples[1], logpdf(Normal(), samples[1]))
```

The probability density function for the normal distribution with mean 0 and standard deviation 1 is

$$p(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2},$$

so we could also have calculated this manually using:

```{julia}
log(1 / sqrt(2π) * exp(-samples[1]^2 / 2))
```

## Sampling from a transformed distribution

Now say that $x$ is distributed according to `Normal()`, and we want to draw samples from $y = \exp(x)$.
The distribution of $y$ is known as a [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution).

For illustration purposes, let's make our own `MyLogNormal` distribution that we can sample from: see Distribution.jl's documentation on custom distributions [here](https://juliastats.org/Distributions.jl/stable/extends/#Univariate-Distribution).
(Distributions already defines its own `LogNormal`, so we have to use a different name.)

```{julia}
struct MyLogNormal <: ContinuousUnivariateDistribution
    μ::Float64
    σ::Float64
end
MyLogNormal() = MyLogNormal(0.0, 1.0)

Base.rand(rng::Random.AbstractRNG, d::MyLogNormal) = exp(rand(rng, Normal(d.μ, d.σ)))
```

Great, now we can do the same as above:

```{julia}
samples_lognormal = rand(MyLogNormal(), 5000)
# Cut off the tail for clearer visualization
histogram(samples_lognormal, bins=0:0.1:5; xlims=(0, 5))
```

How do we implement `logpdf` for our new distribution, though?

Naively, we might think to just un-transform the variable `y`, and then use the `logpdf` of the normal distribution.

```{julia}
bad_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))
```

We can compare this function against the logpdf implemented in Distributions.jl.
(The name chosen here certainly foreshadows that it's not going to be correct, though!)

```{julia}
println("Sample   : $(samples_lognormal[1])")
println("Expected : $(logpdf(LogNormal(), samples_lognormal[1]))")
println("Actual   : $(bad_logpdf(MyLogNormal(), samples_lognormal[1]))")
```

## The derivative

Fundamentally, the reason why this doesn't work is because transforming a (continuous) distribution causes probability density to be stretched and otherwise moved around.

::: {.callout-note}
There are various posts on the Internet that explain this visually; I'm too lazy to draw a diagram _right now_, but I might do it later.
:::

I personally find it most useful to not talk about probability density itself, but instead to make it more concrete by talking about actual probabilities.
If we think about the normal distribution as a continuous curve, what the probability density function $p(x)$ really tells us is that for any two points $a$ and $b$ (where $a \leq b$), the probability of drawing a sample from the interval $[a, b]$ is the area under the curve, i.e.

$$\int_a^b p(x) \, \mathrm{d}x.$$

For example, if $(a, b) = (-\infty, \infty)$, then the probability of drawing a sample from the entire distribution is 1.

Let's say that the probability density function of the log-normal distribution is $q(y)$.
Then, the area under the curve between the two points $\exp(a)$ and $\exp(b)$ is:

$$\int_{\exp(a)}^{\exp(b)} q(y) \, \mathrm{d}y.$$

This integral should be equal to the one above, because the probability of drawing from $[a, b]$ in the original distribution should be the same as the probability of drawing from $[\exp(a), \exp(b)]$ in the transformed distribution.
The question we have to solve here is: how do we find a function $q(y)$ such that this equality holds?

We can approach this by substituting $y = \exp(x)$ into the first integral (see [Wikipedia](https://en.wikipedia.org/wiki/Integration_by_substitution) for a refresher if needed).
We have that:

$$\frac{\mathrm{d}y}{\mathrm{d}x} = \exp(x) = y \implies \mathrm{d}x = \frac{1}{y}\,\mathrm{d}y$$

and so

$$\int_{x=a}^{x=b} p(x) \, \mathrm{d}x
  \longrightarrow \int_{y=\exp(a)}^{y=\exp(b)} \underbrace{p(\log(y)) \frac{1}{y}}_{q(y)} \,\mathrm{d}y,$$

from which we can read off $q(y) = p(\log(y)) / y$.

In contrast, when we implemented `bad_logpdf`

```{julia}
bad_logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y))
```

that was the equivalent of saying that $q(y) = p(\log(y))$.
We left out a factor of $1/y$!

Indeed, now we can define the correct `logpdf` function.
Since everything is a logarithm here, instead of multiplying by $1/y$ we subtract $\log(y)$:

```{julia}
Distributions.logpdf(d::MyLogNormal, y) = logpdf(Normal(d.μ, d.σ), log(y)) - log(y)
```

and check that it works:

```{julia}
println("Sample   : $(samples_lognormal[1])")
println("Expected : $(logpdf(LogNormal(), samples_lognormal[1]))")
println("Actual   : $(logpdf(MyLogNormal(), samples_lognormal[1]))")
```

The same logic can be applied to _any_ kind of transformation.
If we have some transformation from $x$ to $y$, and the probability density functions of $x$ and $y$ are $p(x)$ and $q(y)$ respectively, then

$$q(y) = p(x) \left| \frac{\mathrm{d}x}{\mathrm{d}y} \right|.$$

In this case, we had $y = \exp(x)$, so $\mathrm{d}x/\mathrm{d}y = 1/y$.
This equation is (11.5) in Bishop's textbook.

::: {.callout-note}
The absolute value here takes care of the case where $f$ is decreasing, i.e., the distribution is flipped.
You can try this out with the transformation $y = -\exp(x)$: you will have to flip the integration limits round to ensure that the integral comes out positive.
:::

## The Jacobian


